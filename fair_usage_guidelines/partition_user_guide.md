# üìè Partition Management for Data Scientists

## The Simple Truth About 128MB Partitions

**Question:** "Isn't 128MB the default?"

**Answer:** Yes for READING files, but NO for everything else!

---

## üéØ When You DO and DON'T Need to Act

### ‚úÖ NO ACTION NEEDED (Automatic 128MB)

```python
# Just reading files - Spark handles this automatically
df = spark.read.parquet("s3://bucket/data/")
# ‚úÖ Partitions are already ~128MB each
```

### ‚ö†Ô∏è ACTION REQUIRED (You must fix!)

**After Filtering:**
```python
# Problem:
df = spark.read.parquet("s3://bucket/100gb-data/")  # 800 partitions
df_filtered = df.filter(col("date") == "2024-01-01")  # Now only 1GB!
# ‚ùå Still has 800 partitions! Each is only ~1.25MB (too small!)

# Solution:
df_filtered = df_filtered.coalesce(8)  # 1GB √∑ 128MB = 8 partitions
# ‚úÖ Now 8 partitions √ó 128MB each
```

**Before Aggregations:**
```python
# Problem:
df_agg = df.groupBy("category").agg(sum("amount"))
# ‚ùå Uses default 200 partitions regardless of data size!

# Solution (if expecting 10GB output):
spark.conf.set("spark.sql.shuffle.partitions", "80")  # (10 √ó 1024) √∑ 128
df_agg = df.groupBy("category").agg(sum("amount"))
# ‚úÖ Now ~128MB per partition
```

**Before Joins:**
```python
# Problem:
df_joined = large_df.join(other_df, "id")
# ‚ùå Uses default 200 partitions, might create huge partitions!

# Solution (if expecting 50GB result):
spark.conf.set("spark.sql.shuffle.partitions", "400")  # (50 √ó 1024) √∑ 128
df_joined = large_df.join(other_df, "id")
# ‚úÖ Now ~128MB per partition
```

**Before Writing:**
```python
# Problem:
df.write.parquet("s3://bucket/output/")
# ‚ùå Might create too many tiny files or too few huge files

# Solution (if writing 5GB):
df.coalesce(40).write.parquet("s3://bucket/output/")  # (5 √ó 1024) √∑ 128
# ‚úÖ Creates ~40 files, each ~128MB
```

---

## üìã Quick Decision Tree

```
Did you just READ data?
    ‚Üí NO ACTION (already optimized)

Did you FILTER and reduce data size significantly?
    ‚Üí YES ACTION: coalesce(new_size_GB * 8)

Are you about to groupBy/join?
    ‚Üí YES ACTION: set shuffle.partitions = expected_output_GB * 8

Are you about to WRITE results?
    ‚Üí YES ACTION: coalesce(output_size_GB * 8)
```

*Note: Multiply by 8 is shorthand for (GB √ó 1024) √∑ 128*

---

## üöÄ Easy Mode: Use Adaptive Query Execution

**Add this once at the start of your notebook:**

```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "134217728")  # 128MB
```

**Benefits:**
- Spark automatically optimizes partition sizes during job execution
- You still need to coalesce after major filtering and before writing
- But most other cases are handled automatically!

---

## üìä Simple Formula

```
Partitions = (Data Size in GB √ó 8)
```

**Examples:**
- 1 GB ‚Üí 8 partitions
- 10 GB ‚Üí 80 partitions
- 100 GB ‚Üí 800 partitions

**In Code:**
```python
# After filtering to 10GB:
df = df.coalesce(80)

# Before groupBy expecting 5GB output:
spark.conf.set("spark.sql.shuffle.partitions", "40")

# Before writing 20GB:
df.coalesce(160).write.parquet("...")
```

---

## üîç Check Your Partitions

```python
# See how many partitions you have
print(f"Partitions: {df.rdd.getNumPartitions()}")

# If the number seems wrong for your data size, adjust it!
```

---

## ‚úÖ Complete Example

```python
# 1. Read (automatic 128MB partitions)
df = spark.read.parquet("s3://bucket/100gb-data/")  # ~800 partitions
print(f"Initial: {df.rdd.getNumPartitions()}")

# 2. Filter (data shrinks to 10GB)
df = df.filter(col("date") == "2024-01")
df = df.coalesce(80)  # 10 √ó 8 = 80 partitions
print(f"After filter: {df.rdd.getNumPartitions()}")

# 3. Aggregate (expect 5GB output)
spark.conf.set("spark.sql.shuffle.partitions", "40")  # 5 √ó 8 = 40
df = df.groupBy("category").agg(sum("amount"))
print(f"After groupBy: {df.rdd.getNumPartitions()}")

# 4. Write (5GB output)
df.coalesce(40).write.parquet("s3://bucket/output/")  # 5 √ó 8 = 40 files
```

---

## üéì Key Takeaways

1. **Reading files** ‚Üí Already optimized, do nothing
2. **After filtering** ‚Üí Always check and adjust partitions
3. **Before shuffle ops** ‚Üí Set `spark.sql.shuffle.partitions`
4. **Before writing** ‚Üí Use `coalesce()` to control output files
5. **Easy button** ‚Üí Enable Adaptive Query Execution

**Remember:** The goal is to keep each partition around 128MB throughout your entire pipeline!
