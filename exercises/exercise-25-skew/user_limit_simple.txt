# ═══════════════════════════════════════════════════════════════════════════
# SIMPLE ANSWER: What user-limit-factor Does
# ═══════════════════════════════════════════════════════════════════════════

YOUR QUESTION:
"user-limit-factor is 1, what does increasing from 1 to 100 do?"

ANSWER: It lets you use the FULL CLUSTER instead of just your "fair share"!


═══════════════════════════════════════════════════════════════════════════
THE PROBLEM
═══════════════════════════════════════════════════════════════════════════

user-limit-factor = 1 means:
"Each user gets their fair share ONLY, even if the cluster is empty"

Your Situation:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
YARN sees:     ~3 "active users" (might be you with multiple sessions)
Your limit:    100% / 3 = 33% of cluster
You get:       10 TB out of 30 TB
Wasted:        20 TB sitting IDLE!


Visual:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Cluster (30 TB):  [██████████████████████████████]

With user-limit-factor = 1:
Your job:         [██████████]                      10 TB (33%)
WASTED:                       [████████████████████] 20 TB idle!


═══════════════════════════════════════════════════════════════════════════
THE FIX
═══════════════════════════════════════════════════════════════════════════

user-limit-factor = 100 means:
"User can use up to 100× their fair share if the cluster is available"

After Change:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Your limit:    (100% / 3) × 100 = 3,333% (capped at 100%)
You get:       28.8 TB out of 30 TB
Used well:     96% of cluster!


Visual:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Cluster (30 TB):  [██████████████████████████████]

With user-limit-factor = 100:
Your job:         [█████████████████████████████░] 28.8 TB (96%)
Reserved:                                       [░] 1.2 TB


═══════════════════════════════════════════════════════════════════════════
QUICK FIX (2 minutes)
═══════════════════════════════════════════════════════════════════════════

1. SSH to master:
   ssh -i key.pem hadoop@<master-dns>

2. Edit file:
   sudo nano /etc/hadoop/conf/capacity-scheduler.xml

3. Find and change:
   <n>yarn.scheduler.capacity.root.default.user-limit-factor</n>
   <value>1</value>  →  <value>100</value>

4. Restart YARN:
   sudo systemctl restart hadoop-yarn-resourcemanager
   sleep 30

5. Restart your Spark job:
   spark.stop()
   spark = SparkSession.builder.<configs>.getOrCreate()


═══════════════════════════════════════════════════════════════════════════
WILL THIS BLOCK OTHER USERS?
═══════════════════════════════════════════════════════════════════════════

NO! Here's how it works:

If you're alone:
  You: 30 TB (100% of cluster) ✓
  Others: None
  
If another user submits job:
  YARN automatically shares:
  You: 15 TB (50%)
  Other: 15 TB (50%)
  Both jobs run! ✓

It's "first come, first served, but share when needed"
NOT "monopolize forever"


═══════════════════════════════════════════════════════════════════════════
EXPECTED RESULTS
═══════════════════════════════════════════════════════════════════════════

                BEFORE          AFTER           IMPROVEMENT
                ══════          ═════           ═══════════
Memory          10 TB           28.8 TB         3× more
Executors       85              240             3× more  
Cores           86              3,600           42× more!
Duration        6 hours         8 minutes       45× faster!


═══════════════════════════════════════════════════════════════════════════
THIS IS YOUR BOTTLENECK!
═══════════════════════════════════════════════════════════════════════════

maximum-capacity = 100  ✓ (allows queue to use full cluster)
user-limit-factor = 1   ❌ (limits YOU to 33% even when cluster is empty!)

Change user-limit-factor to 100 → Problem solved!
