# üöÄ Quick Start Guide - Dataset Comparison

## What You Got

A bulletproof solution for comparing 50GB+ Parquet datasets on AWS EMR that **actually works** (unlike df.count() approaches that fail).

## Why Your Current Approach Fails

‚ùå **Full outer joins** on 50GB data = massive shuffle = OOM  
‚ùå **`df.count()`** on large joined DataFrames = driver memory exhaustion  
‚ùå **Collecting results** to driver = instant failure  

## How This Solution Avoids Failures

‚úÖ **Left anti joins** instead of full outer (10x more efficient)  
‚úÖ **`rdd.count()`** instead of `df.count()` (more stable)  
‚úÖ **Write ‚Üí Read ‚Üí Count** pattern (avoids recomputation)  
‚úÖ **Incremental processing** by column (never collects everything)  
‚úÖ **Parquet intermediate results** (enables checkpointing)  

## Get Started in 3 Steps

### Step 1: Configure (2 minutes)

Edit `parquet_dataset_compare.py` lines 409-420:

```python
DS1_PATH = "s3://your-bucket/path/to/dataset1/"
DS2_PATH = "s3://your-bucket/path/to/dataset2/"
KEY_COLUMNS = ["id"]  # or ["customer_id", "date"]
SKIP_COLUMNS = ["updated_at", "processed_date"]
OUTPUT_PATH = "s3://your-bucket/comparison-output/"
```

### Step 2: Submit to EMR (1 minute)

```bash
# Upload script
aws s3 cp parquet_dataset_compare.py s3://your-bucket/scripts/

# Submit job
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 8g \
  --executor-memory 16g \
  --executor-cores 4 \
  --num-executors 10 \
  s3://your-bucket/scripts/parquet_dataset_compare.py
```

Or use the helper script:
```bash
# Edit submit_comparison.sh first
./submit_comparison.sh
```

### Step 3: Review Results

Check your S3 output path for:
- `differences/` - Full comparison results (Parquet)
- `ds1_only/` - Rows only in dataset 1
- `ds2_only/` - Rows only in dataset 2
- `comparison_report.html` - Visual summary

## Expected Runtime

- **50GB datasets**: 15-30 minutes
- **100GB datasets**: 30-60 minutes
- **200GB datasets**: 1-2 hours

(Depends on cluster size and data complexity)

## File Guide

| File | Purpose | When to Use |
|------|---------|-------------|
| **parquet_dataset_compare.py** | Main script | Always - this is your workhorse |
| **config_template.py** | Configuration examples | Copy and customize for your needs |
| **comparison_examples.py** | Customization patterns | When you need special handling |
| **submit_comparison.sh** | EMR automation | To simplify job submission |
| **README.md** | Full documentation | For troubleshooting and tuning |

## Common Configurations

### Daily Transaction Comparison
```python
KEY_COLUMNS = ["transaction_id"]
SKIP_COLUMNS = ["processed_at"]
# Add in main():
ds1 = spark.read.parquet(DS1_PATH).filter(col("date") == "2025-01-15")
ds2 = spark.read.parquet(DS2_PATH).filter(col("date") == "2025-01-15")
```

### Multi-Key Join
```python
KEY_COLUMNS = ["customer_id", "order_id", "date"]
```

### Financial Data with Tolerance
```python
# In _find_differences() method, modify comparison:
diff_cond = F.abs(F.col(col) - F.col(f"{col}_ds2")) > 0.01  # $0.01 tolerance
```

### Compare Only Critical Columns
```python
# After loading datasets in main():
critical_cols = KEY_COLUMNS + ["amount", "status", "quantity"]
ds1 = ds1.select(critical_cols)
ds2 = ds2.select(critical_cols)
```

## Troubleshooting

### Job Still Fails?

**1. Increase shuffle partitions:**
```python
.config("spark.sql.shuffle.partitions", "400")  # Default is 200
```

**2. Use more memory:**
```bash
--executor-memory 32g  # Instead of 16g
```

**3. Check data skew:**
```python
# In Spark UI, look for tasks taking much longer than others
# Solution: Repartition by key columns
ds1 = ds1.repartition(200, *KEY_COLUMNS)
```

**4. Enable compression:**
```python
.config("spark.sql.parquet.compression.codec", "snappy")
```

### Too Slow?

**1. Use date partitioning:**
```python
# Only compare last 7 days
ds1 = spark.read.parquet(DS1_PATH).filter(col("date") >= "2025-01-10")
```

**2. Sample first:**
```python
quick_check = ds1.sample(0.01).count()  # 1% sample
```

**3. Parallelize by partition:**
See `comparison_examples.py` ‚Üí `parallel_comparison_by_partition()`

## What the Output Tells You

```
DS1 Total Rows: 1,234,567,890       ‚Üê Total rows in dataset 1
DS2 Total Rows: 1,234,500,000       ‚Üê Total rows in dataset 2
Rows only in DS1: 67,890            ‚Üê Keys in DS1 but not DS2
Rows only in DS2: 0                 ‚Üê Keys in DS2 but not DS1
Common Rows: 1,234,500,000          ‚Üê Matching keys between datasets
Rows with Differences: 15,432       ‚Üê Rows where column values differ
Total Differences: 45,289           ‚Üê Total column-level differences
```

**Example Analysis:**
- 67,890 rows only in DS1 ‚Üí Might be new records not yet in DS2
- 0 rows only in DS2 ‚Üí DS2 is a subset of DS1
- 15,432 rows with differences ‚Üí 1.25% of common rows have mismatches
- 45,289 total differences ‚Üí Average of 2.9 columns differ per row

## Query Your Results

### Using PySpark:
```python
# Read differences
diffs = spark.read.parquet("s3://bucket/comparison-output/differences/")

# Most common differences
diffs.groupBy("column_name").count().orderBy(desc("count")).show()

# Specific customer
diffs.filter(col("customer_id") == "C12345").show(truncate=False)

# Export to CSV
diffs.filter(col("column_name") == "amount") \
     .coalesce(1).write.csv("s3://bucket/amount_diffs.csv", header=True)
```

### Using AWS Athena:
```sql
-- After creating external table (see README.md)
SELECT column_name, COUNT(*) as diff_count
FROM comparison_diffs
GROUP BY column_name
ORDER BY diff_count DESC;
```

## Performance Tips

1. **Right-size your cluster**: 50GB ‚Üí 10-15 r5.4xlarge nodes
2. **Use columnar Parquet**: Already done ‚úÖ
3. **Partition your data**: By date/region for faster filtering
4. **Enable adaptive execution**: Already configured ‚úÖ
5. **Monitor Spark UI**: Watch for stragglers and data skew

## Next Steps

1. ‚úÖ Run comparison on sample data first
2. ‚úÖ Review HTML report to understand differences
3. ‚úÖ Adjust configuration based on your needs
4. ‚úÖ Set up scheduled comparisons if needed
5. ‚úÖ Integrate results into your data pipeline

## Get Help

- **Job failures**: Check README.md ‚Üí Troubleshooting section
- **Customization**: See comparison_examples.py for 10+ patterns
- **Configuration**: Use config_template.py as starting point
- **Spark UI**: http://your-emr-master:18080 (access via SSH tunnel)

## Key Success Metrics

‚úÖ **Job completes without OOM**  
‚úÖ **Results available in S3 output path**  
‚úÖ **HTML report generated successfully**  
‚úÖ **Runtime under 1 hour for 50GB**  

