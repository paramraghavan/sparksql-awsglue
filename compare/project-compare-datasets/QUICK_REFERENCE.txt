╔══════════════════════════════════════════════════════════════════════════════╗
║                    PARQUET COMPARISON - QUICK REFERENCE                      ║
╚══════════════════════════════════════════════════════════════════════════════╝

┌──────────────────────────────────────────────────────────────────────────────┐
│ BASIC USAGE                                                                  │
└──────────────────────────────────────────────────────────────────────────────┘

# Local Test
python test_local.py --full

# EMR Submission
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 20 \
  --executor-memory 8g \
  s3://bucket/parquet_comparison.py \
  --path1 s3://bucket/file1.parquet \
  --path2 s3://bucket/file2.parquet \
  --keys "id,date" \
  --output s3://bucket/report.html

┌──────────────────────────────────────────────────────────────────────────────┐
│ SIZING GUIDE (50GB+ files)                                                   │
└──────────────────────────────────────────────────────────────────────────────┘

Data Size      Executors    Memory    Cores    Partitions    Est. Time
─────────────────────────────────────────────────────────────────────────
10-25 GB       10-15        8g        4        200-300       10-15 min
25-50 GB       15-20        8g        4        300-400       15-25 min
50-100 GB      20-30        8-12g     4        400-800       25-40 min
100-200 GB     30-50        12g       4-5      800-1200      40-60 min
200+ GB        50+          12-16g    5        1200+         60+ min

┌──────────────────────────────────────────────────────────────────────────────┐
│ CONFIGURATION PRESETS                                                        │
└──────────────────────────────────────────────────────────────────────────────┘

# Small Dataset (< 25GB)
--num-executors 10 \
--executor-cores 4 \
--executor-memory 8g \
--driver-memory 8g \
--partitions 200

# Medium Dataset (25-75GB)
--num-executors 20 \
--executor-cores 4 \
--executor-memory 8g \
--driver-memory 12g \
--partitions 400

# Large Dataset (75-150GB)
--num-executors 30 \
--executor-cores 4 \
--executor-memory 12g \
--driver-memory 12g \
--partitions 800

# Very Large Dataset (150GB+)
--num-executors 50 \
--executor-cores 5 \
--executor-memory 12g \
--driver-memory 16g \
--partitions 1200

┌──────────────────────────────────────────────────────────────────────────────┐
│ COMMON ISSUES & FIXES                                                        │
└──────────────────────────────────────────────────────────────────────────────┘

Issue: Out of Memory
Fix:  ↑ --executor-memory 12g
      ↑ --partitions 800
      ↓ --executor-cores 3

Issue: Slow Performance
Fix:  ↑ --num-executors 40
      ↑ --partitions 1000
      Check data skew in Spark UI

Issue: Executors Lost
Fix:  --conf spark.network.timeout=1200s
      --conf spark.executor.heartbeatInterval=120s
      ↑ --executor-memory

Issue: Too Many Small Files
Fix:  --conf spark.sql.files.maxPartitionBytes=268435456
      Pre-coalesce input data

Issue: Stuck at Certain Percentage
Fix:  Data skew - increase partitions
      Check Spark UI for straggler tasks
      Enable adaptive query execution (already on)

┌──────────────────────────────────────────────────────────────────────────────┐
│ MONITORING                                                                   │
└──────────────────────────────────────────────────────────────────────────────┘

# Check Job Status
aws emr describe-step --cluster-id j-XXX --step-id s-XXX

# SSH to Master Node
aws emr ssh --cluster-id j-XXX --key-pair-file key.pem

# View Spark UI (via SSH tunnel)
ssh -i key.pem -N -L 8157:localhost:8088 hadoop@<master-dns>
Open: http://localhost:8157

# Download Logs
aws s3 sync s3://logs-bucket/j-XXX/ ./logs/

# CloudWatch Metrics to Watch
- YARNMemoryAvailablePercentage (should not hit 0)
- ContainerPending (should not remain high)
- AppsRunning (should be 1 during execution)

┌──────────────────────────────────────────────────────────────────────────────┐
│ KEY COLUMN PATTERNS                                                          │
└──────────────────────────────────────────────────────────────────────────────┘

# Single Natural Key
--keys "transaction_id"

# Composite Business Key
--keys "customer_id,order_date,product_id"

# Temporal Key
--keys "user_id,event_timestamp"

# Surrogate Key + Version
--keys "id,version"

# Multi-level Hierarchy
--keys "country,region,store_id,transaction_id"

┌──────────────────────────────────────────────────────────────────────────────┐
│ SKIP COLUMN PATTERNS                                                         │
└──────────────────────────────────────────────────────────────────────────────┘

# Audit Columns
--skip "created_at,updated_at,modified_by,loaded_at"

# System Columns
--skip "etl_timestamp,pipeline_run_id,source_system"

# Calculated/Derived Fields
--skip "full_name,total_amount,age_calculated"

# Large Text Fields (for performance)
--skip "description,comments,notes,json_payload"

┌──────────────────────────────────────────────────────────────────────────────┐
│ USEFUL SPARK CONFIGS                                                         │
└──────────────────────────────────────────────────────────────────────────────┘

# Already included in script:
--conf spark.sql.adaptive.enabled=true
--conf spark.sql.adaptive.coalescePartitions.enabled=true
--conf spark.sql.adaptive.skewJoin.enabled=true

# For very large datasets, add:
--conf spark.sql.autoBroadcastJoinThreshold=-1
--conf spark.sql.files.maxPartitionBytes=268435456
--conf spark.memory.fraction=0.8
--conf spark.memory.storageFraction=0.3

# For network issues:
--conf spark.network.timeout=1200s
--conf spark.executor.heartbeatInterval=120s
--conf spark.rpc.message.maxSize=512

# For skewed data:
--conf spark.sql.adaptive.skewJoin.skewedPartitionFactor=5
--conf spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=256MB

┌──────────────────────────────────────────────────────────────────────────────┐
│ COST OPTIMIZATION                                                            │
└──────────────────────────────────────────────────────────────────────────────┘

1. Right-size cluster:
   - Start small, scale up if needed
   - Rule: Data size × 4 = Total cluster memory

2. Use Spot Instances:
   - Core nodes: 70% savings
   - Task nodes: 90% savings
   - Master: On-Demand (critical)

3. Auto-terminate:
   --auto-terminate --termination-protected=false

4. Instance Types (ordered by cost/performance):
   - Best: r5.2xlarge (64GB, $0.504/hr On-Demand)
   - Good: m5.2xlarge (32GB, $0.384/hr On-Demand)
   - Budget: m5.xlarge (16GB, $0.192/hr On-Demand)

5. Timing:
   - Run during off-peak hours
   - Batch multiple comparisons

┌──────────────────────────────────────────────────────────────────────────────┐
│ FILE PATHS                                                                   │
└──────────────────────────────────────────────────────────────────────────────┘

parquet_comparison.py     # Main script
submit_comparison.sh      # EMR submission helper
test_local.py            # Local testing
README.md                # Full documentation
requirements.txt         # Python dependencies

Output Locations:
- HTML Report: Specified by --output
- Unique Records: /tmp/{dataset_name}_unique_records/
- Logs: EMR cluster logs location

┌──────────────────────────────────────────────────────────────────────────────┐
│ EXAMPLE COMMANDS                                                             │
└──────────────────────────────────────────────────────────────────────────────┘

# Daily validation
spark-submit parquet_comparison.py \
  --path1 s3://prod/data/date=2024-11-13/ \
  --path2 s3://staging/data/date=2024-11-13/ \
  --keys "id" \
  --skip "loaded_at" \
  --name1 "Prod" --name2 "Stage" \
  --output s3://reports/daily-2024-11-13.html

# Compare partitioned data
spark-submit parquet_comparison.py \
  --path1 "s3://data/year=2024/month=11/*" \
  --path2 "s3://data2/year=2024/month=11/*" \
  --keys "id,timestamp" \
  --partitions 600

# Large dataset with custom config
spark-submit \
  --num-executors 40 \
  --executor-memory 12g \
  --conf spark.sql.shuffle.partitions=1000 \
  parquet_comparison.py \
  --path1 s3://huge/file1.parquet \
  --path2 s3://huge/file2.parquet \
  --keys "composite,key,fields" \
  --skip "audit,fields,timestamps"

┌──────────────────────────────────────────────────────────────────────────────┐
│ REPORT FEATURES                                                              │
└──────────────────────────────────────────────────────────────────────────────┘

✓ Summary dashboard with key metrics
✓ Record count comparison
✓ Schema differences (columns only in each dataset)
✓ Unique records identification
✓ Column-by-column difference counts
✓ Sample differences with actual values
✓ Interactive search and filtering
✓ Collapsible sections for large reports
✓ Print-friendly format

┌──────────────────────────────────────────────────────────────────────────────┐
│ SUPPORT & DEBUGGING                                                          │
└──────────────────────────────────────────────────────────────────────────────┘

1. Check console output for progress
2. Review Spark UI for bottlenecks
3. Examine CloudWatch logs for errors
4. Test with small data subset first
5. Verify key columns exist in both datasets
6. Check S3 permissions

Debug Mode:
spark.sparkContext.setLogLevel("INFO")  # More verbose logging

Sample Testing:
--path1 "s3://bucket/file.parquet limit 10000"  # Can't use LIMIT with parquet
# Instead: Sample in pre-processing or test locally first

╔══════════════════════════════════════════════════════════════════════════════╗
║ Pro Tip: Always test with small data locally before running on full dataset ║
╚══════════════════════════════════════════════════════════════════════════════╝
