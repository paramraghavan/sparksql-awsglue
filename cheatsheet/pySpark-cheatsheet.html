<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PySpark Cheat Sheet</title>
    <style>
        :root {
            --primary: #1F4E79;
            --secondary: #2E75B6;
            --accent: #5B9BD5;
            --bg-code: #f8f9fa;
            --border: #dee2e6;
            --text: #333;
        }
        
        * { box-sizing: border-box; }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.6;
            color: var(--text);
            max-width: 1100px;
            margin: 0 auto;
            padding: 20px;
            background: #fff;
        }
        
        h1 {
            color: var(--primary);
            text-align: center;
            font-size: 2.5em;
            margin-bottom: 5px;
            border-bottom: 3px solid var(--accent);
            padding-bottom: 15px;
        }
        
        .subtitle {
            text-align: center;
            color: #666;
            font-style: italic;
            margin-bottom: 30px;
        }
        
        h2 {
            color: var(--primary);
            border-bottom: 2px solid var(--accent);
            padding-bottom: 8px;
            margin-top: 40px;
        }
        
        h3 {
            color: var(--secondary);
            margin-top: 25px;
        }
        
        h4 {
            color: #444;
            margin-top: 20px;
        }
        
        pre {
            background: var(--bg-code);
            border: 1px solid var(--border);
            border-left: 4px solid var(--accent);
            border-radius: 4px;
            padding: 15px;
            overflow-x: auto;
            font-size: 0.9em;
        }
        
        code {
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            background: var(--bg-code);
            padding: 2px 6px;
            border-radius: 3px;
            font-size: 0.9em;
        }
        
        pre code {
            background: none;
            padding: 0;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.95em;
        }
        
        th {
            background: var(--primary);
            color: white;
            padding: 12px;
            text-align: left;
        }
        
        td {
            padding: 10px 12px;
            border: 1px solid var(--border);
        }
        
        tr:nth-child(even) { background: #f8f9fa; }
        tr:hover { background: #e9ecef; }
        
        .symptom {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 12px 15px;
            margin: 15px 0;
            border-radius: 0 4px 4px 0;
        }
        
        .tip {
            background: #d4edda;
            border-left: 4px solid #28a745;
            padding: 12px 15px;
            margin: 15px 0;
            border-radius: 0 4px 4px 0;
        }
        
        .warning {
            background: #f8d7da;
            border-left: 4px solid #dc3545;
            padding: 12px 15px;
            margin: 15px 0;
            border-radius: 0 4px 4px 0;
        }
        
        ul { padding-left: 25px; }
        li { margin: 8px 0; }
        
        hr {
            border: none;
            border-top: 1px solid var(--border);
            margin: 30px 0;
        }
        
        .toc {
            background: #f8f9fa;
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 20px 30px;
            margin: 30px 0;
        }
        
        .toc h3 { margin-top: 0; color: var(--primary); }
        .toc ul { list-style: none; padding-left: 0; }
        .toc li { margin: 6px 0; }
        .toc a { color: var(--secondary); text-decoration: none; }
        .toc a:hover { text-decoration: underline; }
        
        @media print {
            body { max-width: 100%; }
            pre { white-space: pre-wrap; word-wrap: break-word; }
            h2 { page-break-before: always; }
            h2:first-of-type { page-break-before: avoid; }
        }
        
        @media (max-width: 768px) {
            body { padding: 10px; }
            pre { font-size: 0.8em; }
            table { font-size: 0.85em; }
        }
    </style>
</head>
<body>

<h1>ðŸ”¥ PySpark Cheat Sheet</h1>
<p class="subtitle">For Data Engineers &amp; Data Scientists</p>

<div class="toc">
    <h3>ðŸ“š Table of Contents</h3>
    <ul>
        <li><a href="#part1">Part 1: Local Development Setup</a></li>
        <li><a href="#part2">Part 2: PySpark Fundamentals</a></li>
        <li><a href="#part3">Part 3: Common Transformations</a></li>
        <li><a href="#part4">Part 4: String &amp; Date Functions</a></li>
        <li><a href="#part5">Part 5: Data Engineering Patterns</a></li>
        <li><a href="#part6">Part 6: PySpark ML for Data Scientists</a></li>
        <li><a href="#part7">Part 7: Performance Optimization</a></li>
        <li><a href="#part8">Part 8: EMR Job Troubleshooting Guide</a></li>
        <li><a href="#part9">Part 9: Quick Reference Card</a></li>
    </ul>
</div>

<h2 id="part1">Part 1: Local Development Setup</h2>

<h3>Option A: Docker (Recommended - Works on Windows &amp; Mac)</h3>
<p>The easiest way to run PySpark locally with zero configuration hassles.</p>

<h4>Step 1: Install Docker Desktop</h4>
<ul>
    <li><strong>Windows:</strong> Download from docker.com/products/docker-desktop</li>
    <li><strong>Mac:</strong> Download from docker.com or use: <code>brew install --cask docker</code></li>
</ul>

<h4>Step 2: Run Jupyter with PySpark</h4>
<pre><code># Pull and run the official Jupyter PySpark image
docker run -p 8888:8888 -p 4040:4040 \
  -v $(pwd):/home/jovyan/work \
  jupyter/pyspark-notebook

# For Windows PowerShell, use:
docker run -p 8888:8888 -p 4040:4040 `
  -v ${PWD}:/home/jovyan/work `
  jupyter/pyspark-notebook</code></pre>

<p>Open the URL shown in terminal (e.g., <code>http://127.0.0.1:8888/?token=...</code>) to access Jupyter.</p>

<hr>

<h3>Option B: Native Installation (Windows)</h3>

<h4>Step 1: Install Java 8 or 11</h4>
<pre><code># Using winget (Windows 11/10)
winget install -e --id EclipseAdoptium.Temurin.11.JDK

# Set JAVA_HOME environment variable
# System Properties > Environment Variables > New System Variable
# JAVA_HOME = C:\Program Files\Eclipse Adoptium\jdk-11...</code></pre>

<h4>Step 2: Install Python &amp; PySpark</h4>
<pre><code># Install Python 3.9+ from python.org

# Create virtual environment
python -m venv pyspark_env
pyspark_env\Scripts\activate

# Install PySpark and Jupyter
pip install pyspark jupyterlab pandas pyarrow findspark</code></pre>

<h4>Step 3: Download Hadoop winutils (Windows Only)</h4>
<pre><code># Download winutils.exe for your Hadoop version from:
# https://github.com/steveloughran/winutils

# Create folder: C:\hadoop\bin
# Place winutils.exe there
# Set environment variable: HADOOP_HOME = C:\hadoop</code></pre>

<hr>

<h3>Option C: Native Installation (Mac)</h3>
<pre><code># Install Homebrew if not installed
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Install Java and Apache Spark
brew install openjdk@11 apache-spark

# Install Python packages
pip3 install pyspark jupyterlab pandas pyarrow findspark

# Add to ~/.zshrc or ~/.bash_profile:
export JAVA_HOME=$(/usr/libexec/java_home -v 11)
export SPARK_HOME=/opt/homebrew/Cellar/apache-spark/*/libexec
export PATH=$SPARK_HOME/bin:$PATH</code></pre>

<hr>

<h3>Verify Installation</h3>
<pre><code># In terminal/command prompt
spark-submit --version

# Start Jupyter Lab
jupyter lab</code></pre>

<h2 id="part2">Part 2: PySpark Fundamentals</h2>

<h3>Initialize Spark Session</h3>
<pre><code>from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
from pyspark.sql.window import Window

# Local development session
spark = SparkSession.builder \
    .appName("LocalDev") \
    .master("local[*]") \
    .config("spark.driver.memory", "4g") \
    .config("spark.sql.shuffle.partitions", "8") \
    .getOrCreate()

# Access Spark UI at http://localhost:4040</code></pre>

<h3>Creating DataFrames</h3>
<pre><code># From Python list
data = [(&quot;Alice&quot;, 34, &quot;NYC&quot;), (&quot;Bob&quot;, 45, &quot;LA&quot;)]
df = spark.createDataFrame(data, [&quot;name&quot;, &quot;age&quot;, &quot;city&quot;])

# ---- Sample datasets (from Archive.zip) ----
# 1) Extract Archive.zip next to your notebook as: data/
DATA_DIR = &quot;data&quot;  # &quot;./data&quot; locally, or &quot;/dbfs/FileStore/data&quot; in Databricks

customers = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/customers.csv&quot;))

sales = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/sales.csv&quot;))

products = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/products.csv&quot;))

departments = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/departments.csv&quot;))

employees = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/employees.csv&quot;))

web_logs = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/web_logs.csv&quot;))

skewed = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/skewed_data.csv&quot;))

ml = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/ml_features.csv&quot;))

# JSON (newline-delimited JSON)
users_json = spark.read.json(f&quot;{DATA_DIR}/sample_data.json&quot;)

# Parquet (same datasets are included as parquet too)
customers_pq = spark.read.parquet(f&quot;{DATA_DIR}/customers.parquet&quot;)</code></pre>

<h3>DataFrame Inspection</h3>
<pre><code>df.show(10)                    # Display first 10 rows
df.show(truncate=False)        # Show full content
df.printSchema()               # Show schema with data types
df.dtypes                      # List of (column, type) tuples
df.columns                     # List of column names
df.count()                     # Number of rows
df.describe().show()           # Summary statistics
df.explain()                   # Show execution plan
df.explain(True)               # Detailed execution plan</code></pre>

<h3>Column Selection &amp; Manipulation</h3>
<pre><code># Select columns
df.select("name", "age")
df.select(F.col("name"), F.col("age"))
df.select(df["name"], df.age)

# Select with expressions
df.select(
    F.col("name"),
    (F.col("age") + 10).alias("age_plus_10"),
    F.upper(F.col("name")).alias("name_upper")
)

# Add new columns
df.withColumn("new_col", F.lit("constant"))
df.withColumn("age_doubled", F.col("age") * 2)

# Rename columns
df.withColumnRenamed("old_name", "new_name")

# Drop columns
df.drop("column1", "column2")</code></pre>

<h2 id="part3">Part 3: Common Transformations</h2>

<h3>Filtering Data</h3>
<pre><code># Basic filters
df.filter(F.col("age") > 30)
df.filter("age > 30")                    # SQL expression
df.where(F.col("city") == "NYC")

# Multiple conditions
df.filter((F.col("age") > 30) & (F.col("city") == "NYC"))
df.filter((F.col("age") < 25) | (F.col("age") > 60))

# IN clause
df.filter(F.col("city").isin(["NYC", "LA", "Chicago"]))

# NULL handling
df.filter(F.col("name").isNotNull())
df.filter(F.col("name").isNull())

# String matching
df.filter(F.col("name").like("%Alice%"))
df.filter(F.col("name").rlike("^A.*"))  # Regex</code></pre>

<h3>Aggregations</h3>
<pre><code># Basic aggregations
df.groupBy("city").count()
df.groupBy("city").agg(
    F.count("*").alias("total"),
    F.avg("age").alias("avg_age"),
    F.max("age").alias("max_age"),
    F.min("age").alias("min_age"),
    F.sum("salary").alias("total_salary"),
    F.countDistinct("department").alias("unique_depts")
)

# Multiple grouping columns
df.groupBy("city", "department").agg(...)

# Collect values into list
df.groupBy("city").agg(
    F.collect_list("name").alias("names"),
    F.collect_set("department").alias("unique_depts")
)</code></pre>

<h3>Joins</h3>
<pre><code># Sample dataset example
# sales enriched with customer + product + sales rep attributes
sales_enriched = (sales
    .join(customers.select(&quot;customer_id&quot;, &quot;name&quot;, &quot;city&quot;, &quot;state&quot;, &quot;tier&quot;), on=&quot;customer_id&quot;, how=&quot;left&quot;)
    .join(products.select(&quot;product&quot;, &quot;category&quot;, &quot;msrp&quot;, &quot;cost&quot;), on=&quot;product&quot;, how=&quot;left&quot;)
    .join(employees.select(F.col(&quot;id&quot;).alias(&quot;sales_rep_id&quot;), &quot;first_name&quot;, &quot;last_name&quot;, &quot;department&quot;),
          on=&quot;sales_rep_id&quot;, how=&quot;left&quot;)
)

# Anti-join: customers with no purchases
customers_no_sales = customers.join(sales.select(&quot;customer_id&quot;).distinct(), on=&quot;customer_id&quot;, how=&quot;left_anti&quot;)

# Quick reference for join types
df1.join(df2, on=&quot;id&quot;, how=&quot;inner&quot;)   # inner (default)
df1.join(df2, on=&quot;id&quot;, how=&quot;left&quot;)    # left
df1.join(df2, on=&quot;id&quot;, how=&quot;right&quot;)   # right
df1.join(df2, on=&quot;id&quot;, how=&quot;outer&quot;)   # full outer
df1.join(df2, on=&quot;id&quot;, how=&quot;left_semi&quot;)
df1.join(df2, on=&quot;id&quot;, how=&quot;left_anti&quot;)</code></pre>

<h3>Window Functions</h3>
<pre><code>from pyspark.sql.window import Window

# Define window specification
window_spec = Window.partitionBy("department").orderBy(F.desc("salary"))

# Ranking functions
df.withColumn("rank", F.rank().over(window_spec))
df.withColumn("dense_rank", F.dense_rank().over(window_spec))
df.withColumn("row_number", F.row_number().over(window_spec))
df.withColumn("ntile", F.ntile(4).over(window_spec))  # Quartiles

# Analytic functions
df.withColumn("prev_salary", F.lag("salary", 1).over(window_spec))
df.withColumn("next_salary", F.lead("salary", 1).over(window_spec))
df.withColumn("first_val", F.first("salary").over(window_spec))
df.withColumn("last_val", F.last("salary").over(window_spec))

# Running totals
running_window = Window.partitionBy("dept").orderBy("date") \
    .rowsBetween(Window.unboundedPreceding, Window.currentRow)
df.withColumn("running_total", F.sum("amount").over(running_window))</code></pre>

<h2 id="part4">Part 4: String &amp; Date Functions</h2>

<h3>String Functions</h3>
<pre><code># Case conversion (sample: customers)
customers.select(F.upper(&quot;name&quot;), F.lower(&quot;name&quot;), F.initcap(&quot;name&quot;)).show(5, truncate=False)

# Trim and padding
customers.select(
    F.trim(&quot;name&quot;).alias(&quot;name_trim&quot;),
    F.lpad(F.col(&quot;customer_id&quot;).cast(&quot;string&quot;), 8, &quot;0&quot;).alias(&quot;customer_id_padded&quot;)
).show(5, truncate=False)

# Substring and length
customers.select(
    &quot;name&quot;,
    F.substring(&quot;name&quot;, 1, 3).alias(&quot;name_prefix&quot;),
    F.length(&quot;name&quot;).alias(&quot;name_len&quot;)
).show(5, truncate=False)

# Split and explode (customers.tags is a comma-separated string)
customers.select(F.split(&quot;tags&quot;, &quot;,&quot;).alias(&quot;tag_array&quot;)).show(5, truncate=False)
customers.select(F.explode(F.split(&quot;tags&quot;, &quot;,&quot;)).alias(&quot;tag&quot;)).groupBy(&quot;tag&quot;).count().show()

# Regex examples
customers.select(
    F.regexp_extract(&quot;email&quot;, r&quot;(.+)@(.+)&quot;, 1).alias(&quot;email_user&quot;),
    F.regexp_replace(&quot;phone&quot;, r&quot;[^0-9]&quot;, &quot;&quot;).alias(&quot;clean_phone&quot;)
).show(5, truncate=False)

web_logs.select(
    &quot;page&quot;,
    F.regexp_extract(&quot;page&quot;, r&quot;^/([^/]+)&quot;, 1).alias(&quot;route&quot;)
).show(5, truncate=False)</code></pre>

<h3>Date &amp; Timestamp Functions</h3>
<pre><code># Current date/time
spark.range(1).select(F.current_date().alias(&quot;today&quot;), F.current_timestamp().alias(&quot;now&quot;)).show()

# Parse strings to dates/timestamps (sample datasets)
sales2 = sales.withColumn(&quot;sale_date&quot;, F.to_date(&quot;date&quot;, &quot;yyyy-MM-dd&quot;))
logs2 = web_logs.withColumn(&quot;ts&quot;, F.to_timestamp(&quot;timestamp&quot;))

# Extract components
sales2.select(
    &quot;sale_id&quot;,
    &quot;sale_date&quot;,
    F.year(&quot;sale_date&quot;).alias(&quot;year&quot;),
    F.month(&quot;sale_date&quot;).alias(&quot;month&quot;),
    F.dayofmonth(&quot;sale_date&quot;).alias(&quot;day&quot;)
).show(5)

logs2.select(
    &quot;log_id&quot;,
    &quot;ts&quot;,
    F.hour(&quot;ts&quot;).alias(&quot;hour&quot;),
    F.minute(&quot;ts&quot;).alias(&quot;minute&quot;)
).show(5)

# Date arithmetic
sales2.select(
    &quot;sale_date&quot;,
    F.date_add(&quot;sale_date&quot;, 7).alias(&quot;plus_7d&quot;),
    F.date_sub(&quot;sale_date&quot;, 30).alias(&quot;minus_30d&quot;)
).show(5)

# Format dates
sales2.select(F.date_format(&quot;sale_date&quot;, &quot;MMM dd, yyyy&quot;).alias(&quot;pretty_date&quot;)).show(5, truncate=False)</code></pre>

<h2 id="part5">Part 5: Data Engineering Patterns</h2>

<h3>Handling Nulls &amp; Duplicates</h3>
<pre><code># Fill nulls
df.na.fill(0)                          # Fill all nulls with 0
df.na.fill({"age": 0, "name": "Unknown"})
df.fillna({"salary": df.agg(F.avg("salary")).first()[0]})

# Drop nulls
df.na.drop()                           # Drop rows with any null
df.na.drop("all")                      # Drop only if all values null
df.na.drop(subset=["name", "age"])     # Check specific columns

# Coalesce (return first non-null)
df.select(F.coalesce("preferred_name", "name").alias("display_name"))

# Remove duplicates
df.dropDuplicates()                    # All columns
df.dropDuplicates(["email"])           # Based on specific columns

# Keep first/last duplicate based on ordering
window = Window.partitionBy("email").orderBy(F.desc("created_at"))
df.withColumn("rn", F.row_number().over(window)) \
  .filter(F.col("rn") == 1).drop("rn")</code></pre>

<h3>Schema Definition &amp; Enforcement</h3>
<pre><code>from pyspark.sql.types import *

# Define explicit schema
schema = StructType([
    StructField("id", LongType(), nullable=False),
    StructField("name", StringType(), nullable=True),
    StructField("age", IntegerType(), nullable=True),
    StructField("salary", DoubleType(), nullable=True),
    StructField("hire_date", DateType(), nullable=True),
    StructField("metadata", MapType(StringType(), StringType())),
    StructField("tags", ArrayType(StringType())),
    StructField("address", StructType([
        StructField("city", StringType()),
        StructField("zip", StringType())
    ]))
])

# Read with schema (faster than inferSchema)
df = spark.read.schema(schema).json("data.json")

# Cast column types
df.withColumn("age", F.col("age").cast(IntegerType()))
df.withColumn("amount", F.col("amount").cast("decimal(10,2)"))</code></pre>

<h3>Writing Data</h3>
<pre><code># Write to Parquet (recommended)
df.write.mode("overwrite").parquet("output/data.parquet")

# Write modes: overwrite, append, ignore, error (default)
df.write.mode("append").parquet("output/")

# Partitioned write (critical for large datasets)
df.write \
    .mode("overwrite") \
    .partitionBy("year", "month") \
    .parquet("output/partitioned/")

# Control number of output files
df.coalesce(1).write.csv("single_file/")   # Single file
df.repartition(10).write.parquet("output/") # 10 files

# Write to CSV with options
df.write \
    .mode("overwrite") \
    .option("header", True) \
    .option("delimiter", ",") \
    .csv("output.csv")

# Write to Delta (if using Delta Lake)
df.write.format("delta").mode("overwrite").save("delta_table/")</code></pre>

<h2 id="part6">Part 6: PySpark ML for Data Scientists</h2>

<h3>Feature Engineering</h3>
<pre><code>from pyspark.ml.feature import (
    VectorAssembler, StandardScaler, StringIndexer,
    OneHotEncoder, Bucketizer, Imputer
)

# Combine features into vector
assembler = VectorAssembler(
    inputCols=["age", "salary", "experience"],
    outputCol="features"
)
df = assembler.transform(df)

# Scale features
scaler = StandardScaler(
    inputCol="features",
    outputCol="scaled_features",
    withMean=True, withStd=True
)
scaler_model = scaler.fit(df)
df = scaler_model.transform(df)

# Encode categorical variables
indexer = StringIndexer(inputCol="category", outputCol="category_idx")
encoder = OneHotEncoder(inputCol="category_idx", outputCol="category_vec")

# Handle missing values
imputer = Imputer(
    inputCols=["age", "salary"],
    outputCols=["age_imputed", "salary_imputed"],
    strategy="median"
)</code></pre>

<h3>ML Pipeline Example</h3>
<pre><code>from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler

# Sample dataset: ml_features.csv (loaded as `ml`)
df = ml.dropna(subset=[&quot;label&quot;])

train, test = df.randomSplit([0.8, 0.2], seed=42)

cat_cols = [&quot;category&quot;, &quot;region&quot;]
num_cols = [&quot;age&quot;, &quot;income&quot;, &quot;credit_score&quot;, &quot;years_employed&quot;, &quot;num_accounts&quot;]

indexers = [StringIndexer(inputCol=c, outputCol=f&quot;{c}_idx&quot;, handleInvalid=&quot;keep&quot;) for c in cat_cols]
encoders = [OneHotEncoder(inputCol=f&quot;{c}_idx&quot;, outputCol=f&quot;{c}_vec&quot;) for c in cat_cols]

assembler = VectorAssembler(
    inputCols=num_cols + [f&quot;{c}_vec&quot; for c in cat_cols],
    outputCol=&quot;features&quot;
)

rf = RandomForestClassifier(labelCol=&quot;label&quot;, featuresCol=&quot;features&quot;, numTrees=200, seed=42)

pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])

model = pipeline.fit(train)
pred = model.transform(test)

auc = BinaryClassificationEvaluator(labelCol=&quot;label&quot;).evaluate(pred)
print(&quot;AUC:&quot;, auc)</code></pre>

<h3>Pandas UDFs for Advanced Analytics</h3>
<pre><code>import pandas as pd
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import DoubleType

# Scalar UDF - applied row by row (vectorized)
@pandas_udf(DoubleType())
def normalize(series: pd.Series) -> pd.Series:
    return (series - series.mean()) / series.std()

df.select(normalize("salary").alias("normalized_salary"))

# Grouped Map UDF - apply function to each group
@pandas_udf(df.schema, functionType=PandasUDFType.GROUPED_MAP)
def subtract_mean(pdf: pd.DataFrame) -> pd.DataFrame:
    pdf["salary"] = pdf["salary"] - pdf["salary"].mean()
    return pdf

df.groupby("department").apply(subtract_mean)</code></pre>

<h2 id="part7">Part 7: Performance Optimization</h2>

<h3>Caching &amp; Persistence</h3>
<pre><code>from pyspark import StorageLevel

# Cache in memory (use when DF is reused multiple times)
df.cache()            # Same as persist(StorageLevel.MEMORY_ONLY)
df.persist(StorageLevel.MEMORY_AND_DISK)

# Unpersist when done
df.unpersist()

# Check if cached
df.is_cached

# Force evaluation and cache
df.cache().count()    # Triggers computation and caches</code></pre>

<h3>Partitioning Strategies</h3>
<pre><code># Check current partitions
df.rdd.getNumPartitions()

# Repartition (shuffle - expensive but even distribution)
df.repartition(200)                    # By number
df.repartition("key_column")           # By column (hash)
df.repartition(200, "key_column")      # Both

# Coalesce (no shuffle - for reducing partitions)
df.coalesce(10)                        # Combine into fewer

# Random repartition (for skew issues)
df.withColumn("salt", F.rand()) \
  .repartition(200, "salt") \
  .drop("salt")

# Partition size recommendation: 128 MB per partition
# Calculate: total_data_size_MB / 128 = num_partitions</code></pre>

<h3>Broadcast Joins</h3>
<pre><code>from pyspark.sql.functions import broadcast

# Force broadcast (small table joins large table)
# Best when small table < 10MB (configurable)
result = large_df.join(
    broadcast(small_df),
    "join_key"
)

# Configure auto broadcast threshold (default 10MB)
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 100*1024*1024)  # 100MB</code></pre>

<h3>Key Configuration Parameters</h3>
<table>
    <tr>
        <th>Parameter</th>
        <th>Default</th>
        <th>Recommendation</th>
    </tr>
    <tr>
        <td><code>spark.sql.shuffle.partitions</code></td>
        <td>200</td>
        <td>Set to data_size_GB Ã— 4 (target 128MB/partition)</td>
    </tr>
    <tr>
        <td><code>spark.default.parallelism</code></td>
        <td>Total cores</td>
        <td>2-3x total cores for better parallelism</td>
    </tr>
    <tr>
        <td><code>spark.sql.adaptive.enabled</code></td>
        <td>true (3.0+)</td>
        <td>Enable for automatic optimization</td>
    </tr>
    <tr>
        <td><code>spark.executor.memory</code></td>
        <td>1g</td>
        <td>4-8g per executor typically</td>
    </tr>
    <tr>
        <td><code>spark.executor.cores</code></td>
        <td>1</td>
        <td>4-5 cores per executor is optimal</td>
    </tr>
    <tr>
        <td><code>spark.memory.fraction</code></td>
        <td>0.6</td>
        <td>Increase to 0.8 for memory-heavy jobs</td>
    </tr>
</table>

<h2 id="part8">Part 8: EMR Job Troubleshooting Guide</h2>

<p><em>Common issues encountered when running PySpark jobs on AWS EMR and their solutions.</em></p>

<h3>Issue 1: Data Skew - Uneven Partition Sizes</h3>

<div class="symptom">
    <strong>Symptom:</strong> One or few tasks take much longer than others. Some executors are idle while others are overloaded. Job appears stuck at 99% for a long time.
</div>

<h4>Diagnosis</h4>
<pre><code># Sample dataset: skewed_data.csv (loaded as `skewed`)
from pyspark.sql.functions import spark_partition_id

# Check partition sizes
(skewed.groupBy(spark_partition_id().alias(&quot;partition_id&quot;))
  .count()
  .orderBy(F.desc(&quot;count&quot;))
  .show(50))

# Check for skewed keys
(skewed.groupBy(&quot;join_key&quot;).count()
  .orderBy(F.desc(&quot;count&quot;))
  .show(20))</code></pre>

<h4>Solution: Salted Repartition</h4>
<pre><code># Add random salt and repartition evenly
from pyspark.sql.functions import rand, floor

num_partitions = 500  # Adjust based on data size

df_rebalanced = df \
    .withColumn("_salt", floor(rand() * num_partitions).cast("int")) \
    .repartition(num_partitions, "_salt") \
    .drop("_salt")

# For skewed joins, salt both sides
salt_buckets = 10

# Salt the large/skewed table
df_large_salted = df_large \
    .withColumn("_salt", floor(rand() * salt_buckets).cast("int"))

# Explode the small table to match all salts
df_small_exploded = df_small \
    .withColumn("_salt", F.explode(F.array([F.lit(i) for i in range(salt_buckets)])))

# Join on original key + salt
result = df_large_salted.join(
    df_small_exploded,
    ["join_key", "_salt"],
    "inner"
).drop("_salt")</code></pre>

<hr>

<h3>Issue 2: Jobs Taking Too Long - Insufficient Parallelism</h3>

<div class="symptom">
    <strong>Symptom:</strong> Jobs run slowly even with large clusters. Spark UI shows few active tasks. Shuffle operations are bottlenecked.
</div>

<h4>Root Cause</h4>
<p>Default shuffle partitions (200) is too low for large datasets, resulting in partitions that are too large (> 200MB each).</p>

<h4>Solution: Increase Shuffle Partitions</h4>
<pre><code># Calculate optimal partitions: target 128MB per partition
# For 100GB dataset: 100 * 1024 / 128 = 800 partitions

# Set at session level
spark.conf.set("spark.sql.shuffle.partitions", 800)

# Or in spark-submit
spark-submit \
    --conf spark.sql.shuffle.partitions=800 \
    --conf spark.default.parallelism=800 \
    your_job.py

# Enable Adaptive Query Execution (Spark 3.0+)
spark.conf.set("spark.sql.adaptive.enabled", True)
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", True)
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", True)</code></pre>

<hr>

<h3>Issue 3: Out of Memory Errors</h3>

<div class="symptom">
    <strong>Symptom:</strong> java.lang.OutOfMemoryError, Container killed by YARN for exceeding memory limits, GC overhead limit exceeded.
</div>

<h4>Solutions</h4>
<pre><code># 1. Increase executor memory and overhead
spark-submit \
    --executor-memory 8g \
    --conf spark.executor.memoryOverhead=2g \
    --conf spark.memory.fraction=0.8 \
    your_job.py

# 2. Reduce executor cores (less concurrent tasks per executor)
--executor-cores 4  # Instead of 5

# 3. Increase partitions to reduce per-partition memory
spark.conf.set("spark.sql.shuffle.partitions", 1000)

# 4. Avoid collect() on large datasets
# BAD:
all_data = df.collect()  # Brings all data to driver!

# GOOD:
df.write.parquet("output/")  # Write to storage instead

# 5. Use approximate distinct instead of exact
df.select(F.approx_count_distinct("user_id"))  # Instead of countDistinct</code></pre>

<hr>

<h3>Issue 4: Slow Job Startup / Long Scheduling Delays</h3>

<div class="symptom">
    <strong>Symptom:</strong> Jobs pending for long time before starting. Spark UI shows scheduling delay.
</div>

<h4>Solutions</h4>
<pre><code># 1. Reduce number of small files (small files problem)
# Compact input files first
df = spark.read.parquet("input/")
df.coalesce(100).write.parquet("compacted/")

# 2. Increase YARN resources
# In EMR cluster configuration:
# yarn.nodemanager.resource.memory-mb: 60000
# yarn.scheduler.maximum-allocation-mb: 60000

# 3. Use fewer, larger executors
--num-executors 20 \
--executor-memory 16g \
--executor-cores 5

# 4. Enable dynamic allocation
--conf spark.dynamicAllocation.enabled=true \
--conf spark.dynamicAllocation.minExecutors=10 \
--conf spark.dynamicAllocation.maxExecutors=100</code></pre>

<hr>

<h3>Issue 5: Shuffle Spill to Disk</h3>

<div class="symptom">
    <strong>Symptom:</strong> Excessive shuffle read/write. Jobs slow due to disk I/O. Spark UI shows 'Shuffle Spill (Disk)'.
</div>

<h4>Solutions</h4>
<pre><code># 1. Increase memory for shuffle
spark.conf.set("spark.memory.fraction", 0.8)  # Default 0.6
spark.conf.set("spark.memory.storageFraction", 0.3)  # Default 0.5

# 2. Reduce shuffle data by filtering early
# BAD: Filter after join
result = df1.join(df2, "key").filter("date > '2024-01-01'")

# GOOD: Filter before join
df1_filtered = df1.filter("date > '2024-01-01'")
result = df1_filtered.join(df2, "key")

# 3. Use broadcast for small dimension tables
from pyspark.sql.functions import broadcast
result = large_df.join(broadcast(small_dim), "key")

# 4. Select only needed columns before shuffle
df.select("key", "needed_col1", "needed_col2") \
  .groupBy("key").agg(...)</code></pre>

<hr>

<h3>EMR Best Practices Summary</h3>
<table>
    <tr>
        <th>Scenario</th>
        <th>Configuration</th>
    </tr>
    <tr>
        <td>General ETL</td>
        <td>shuffle.partitions = data_size_GB Ã— 4, executor-memory 8g, executor-cores 5</td>
    </tr>
    <tr>
        <td>Skewed Joins</td>
        <td>Enable AQE: adaptive.skewJoin.enabled = true, or use salting technique</td>
    </tr>
    <tr>
        <td>Many Small Files</td>
        <td>Pre-compact to 128-256MB files, use coalesce() in writes</td>
    </tr>
    <tr>
        <td>Large Aggregations</td>
        <td>Increase shuffle.partitions, use 2-stage aggregation for high cardinality</td>
    </tr>
    <tr>
        <td>Memory Issues</td>
        <td>Reduce executor-cores to 4, increase memoryOverhead, avoid collect()</td>
    </tr>
</table>

<h2 id="part9">Part 9: Quick Reference Card</h2>

<h3>Common Import Template</h3>
<pre><code>from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
from pyspark.sql.window import Window
from pyspark.sql.functions import (
    col, lit, when, coalesce, concat, concat_ws,
    sum, avg, count, max, min, countDistinct,
    year, month, dayofmonth, date_format, to_date,
    split, explode, array, struct, collect_list,
    row_number, rank, dense_rank, lead, lag,
    broadcast, spark_partition_id, rand
)</code></pre>

<h3>Spark UI Tabs Reference</h3>
<table>
    <tr>
        <th>Tab</th>
        <th>What to Look For</th>
    </tr>
    <tr>
        <td>Jobs</td>
        <td>Failed jobs, skipped stages (good - means data was cached)</td>
    </tr>
    <tr>
        <td>Stages</td>
        <td>Task duration variance, shuffle read/write sizes, skew indicators</td>
    </tr>
    <tr>
        <td>Storage</td>
        <td>Cached DataFrames, memory usage per partition</td>
    </tr>
    <tr>
        <td>Executors</td>
        <td>GC time (>10% is bad), shuffle spill, failed tasks</td>
    </tr>
    <tr>
        <td>SQL</td>
        <td>Query plans, join strategies (broadcast vs shuffle), scan sizes</td>
    </tr>
</table>

<h3>Useful Spark Shell Commands</h3>
<pre><code># Start PySpark shell with custom config
pyspark --master local[4] \
        --driver-memory 4g \
        --conf spark.sql.shuffle.partitions=8

# Start with Jupyter notebook
PYSPARK_DRIVER_PYTHON=jupyter \
PYSPARK_DRIVER_PYTHON_OPTS='notebook' \
pyspark

# Submit job to EMR
spark-submit \
    --master yarn \
    --deploy-mode cluster \
    --num-executors 20 \
    --executor-memory 8g \
    --executor-cores 5 \
    --conf spark.sql.shuffle.partitions=500 \
    --conf spark.sql.adaptive.enabled=true \
    s3://bucket/scripts/my_job.py</code></pre>

<div class="tip">
    <strong>ðŸ’¡ Pro Tip:</strong> Always check the Spark UI (port 4040) to understand your job's behavior. The SQL tab shows execution plans that reveal exactly how Spark processes your queries.
</div>

<hr>
<p style="text-align: center; color: #666; margin-top: 40px;">
    <em>PySpark Cheat Sheet â€” For Data Engineers &amp; Data Scientists</em>
</p>

</body>
</html>
