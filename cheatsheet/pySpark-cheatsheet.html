<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PySpark Cheat Sheet</title>
    <style>
        :root {
            --primary: #1F4E79;
            --secondary: #2E75B6;
            --accent: #5B9BD5;
            --bg-code: #f8f9fa;
            --border: #dee2e6;
            --text: #333;
        }
        
        * { box-sizing: border-box; }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.6;
            color: var(--text);
            max-width: 1100px;
            margin: 0 auto;
            padding: 20px;
            background: #fff;
        }
        
        h1 {
            color: var(--primary);
            text-align: center;
            font-size: 2.5em;
            margin-bottom: 5px;
            border-bottom: 3px solid var(--accent);
            padding-bottom: 15px;
        }
        
        .subtitle {
            text-align: center;
            color: #666;
            font-style: italic;
            margin-bottom: 30px;
        }
        
        h2 {
            color: var(--primary);
            border-bottom: 2px solid var(--accent);
            padding-bottom: 8px;
            margin-top: 40px;
        }
        
        h3 {
            color: var(--secondary);
            margin-top: 25px;
        }
        
        h4 {
            color: #444;
            margin-top: 20px;
        }
        
        pre {
            background: var(--bg-code);
            border: 1px solid var(--border);
            border-left: 4px solid var(--accent);
            border-radius: 4px;
            padding: 15px;
            overflow-x: auto;
            font-size: 0.9em;
        }
        
        code {
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            background: var(--bg-code);
            padding: 2px 6px;
            border-radius: 3px;
            font-size: 0.9em;
        }
        
        pre code {
            background: none;
            padding: 0;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.95em;
        }
        
        th {
            background: var(--primary);
            color: white;
            padding: 12px;
            text-align: left;
        }
        
        td {
            padding: 10px 12px;
            border: 1px solid var(--border);
        }
        
        tr:nth-child(even) { background: #f8f9fa; }
        tr:hover { background: #e9ecef; }
        
        .symptom {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 12px 15px;
            margin: 15px 0;
            border-radius: 0 4px 4px 0;
        }
        
        .tip {
            background: #d4edda;
            border-left: 4px solid #28a745;
            padding: 12px 15px;
            margin: 15px 0;
            border-radius: 0 4px 4px 0;
        }
        
        .warning {
            background: #f8d7da;
            border-left: 4px solid #dc3545;
            padding: 12px 15px;
            margin: 15px 0;
            border-radius: 0 4px 4px 0;
        }
        
        ul { padding-left: 25px; }
        li { margin: 8px 0; }
        
        hr {
            border: none;
            border-top: 1px solid var(--border);
            margin: 30px 0;
        }
        
        .toc {
            background: #f8f9fa;
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 20px 30px;
            margin: 30px 0;
        }
        
        .toc h3 { margin-top: 0; color: var(--primary); }
        .toc ul { list-style: none; padding-left: 0; }
        .toc li { margin: 6px 0; }
        .toc a { color: var(--secondary); text-decoration: none; }
        .toc a:hover { text-decoration: underline; }
        
        @media print {
            body { max-width: 100%; }
            pre { white-space: pre-wrap; word-wrap: break-word; }
            h2 { page-break-before: always; }
            h2:first-of-type { page-break-before: avoid; }
        }
        
        @media (max-width: 768px) {
            body { padding: 10px; }
            pre { font-size: 0.8em; }
            table { font-size: 0.85em; }
        }
    </style>
</head>
<body>

<h1>üî• PySpark Cheat Sheet</h1>
<p class="subtitle">For Data Engineers &amp; Data Scientists</p>

<div class="toc">
    <h3>üìö Table of Contents</h3>
    <ul>
        <li><a href="#part1">Part 1: Local Development Setup</a></li>
        <li><a href="#part2">Part 2: PySpark Fundamentals</a></li>
        <li><a href="#part3">Part 3: Common Transformations</a></li>
        <li><a href="#part4">Part 4: String &amp; Date Functions</a></li>
        <li><a href="#part5">Part 5: Data Engineering Patterns</a></li>
        <li><a href="#part6">Part 6: PySpark ML for Data Scientists</a></li>
        <li><a href="#part7">Part 7: Performance Optimization</a></li>
        <li><a href="#part8">Part 8: EMR Job Troubleshooting Guide</a></li>
        <li><a href="#part9">Part 9: Quick Reference Card</a></li>
        <li><a href="#part10">Part 10: Core Concepts &amp; Interview Questions</a></li>
    </ul>
</div>

<h2 id="part1">Part 1: Local Development Setup</h2>

<h3>Option A: Docker (Recommended - Works on Windows &amp; Mac)</h3>
<p>The easiest way to run PySpark locally with zero configuration hassles.</p>

<h4>Step 1: Install Docker Desktop</h4>
<ul>
    <li><strong>Windows:</strong> Download from docker.com/products/docker-desktop</li>
    <li><strong>Mac:</strong> Download from docker.com or use: <code>brew install --cask docker</code></li>
</ul>

<h4>Step 2: Run Jupyter with PySpark</h4>
<pre><code># Pull and run the official Jupyter PySpark image
docker run -p 8888:8888 -p 4040:4040 \
  -v $(pwd):/home/jovyan/work \
  jupyter/pyspark-notebook

# For Windows PowerShell, use:
docker run -p 8888:8888 -p 4040:4040 `
  -v ${PWD}:/home/jovyan/work `
  jupyter/pyspark-notebook</code></pre>

<p>Open the URL shown in terminal (e.g., <code>http://127.0.0.1:8888/?token=...</code>) to access Jupyter.</p>

<hr>

<h3>Option B: Native Installation (Windows)</h3>

<h4>Step 1: Install Java 8 or 11</h4>
<pre><code># Using winget (Windows 11/10)
winget install -e --id EclipseAdoptium.Temurin.11.JDK

# Set JAVA_HOME environment variable
# System Properties > Environment Variables > New System Variable
# JAVA_HOME = C:\Program Files\Eclipse Adoptium\jdk-11...</code></pre>

<h4>Step 2: Install Python &amp; PySpark</h4>
<pre><code># Install Python 3.9+ from python.org

# Create virtual environment
python -m venv pyspark_env
pyspark_env\Scripts\activate

# Install PySpark and Jupyter
pip install pyspark jupyterlab pandas pyarrow findspark</code></pre>

<h4>Step 3: Download Hadoop winutils (Windows Only)</h4>
<pre><code># Download winutils.exe for your Hadoop version from:
# https://github.com/steveloughran/winutils

# Create folder: C:\hadoop\bin
# Place winutils.exe there
# Set environment variable: HADOOP_HOME = C:\hadoop</code></pre>

<hr>

<h3>Option C: Native Installation (Mac)</h3>
<pre><code># Install Homebrew if not installed
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Install Java and Apache Spark
brew install openjdk@11 apache-spark

# Install Python packages
pip3 install pyspark jupyterlab pandas pyarrow findspark

# Add to ~/.zshrc or ~/.bash_profile:
export JAVA_HOME=$(/usr/libexec/java_home -v 11)
export SPARK_HOME=/opt/homebrew/Cellar/apache-spark/*/libexec
export PATH=$SPARK_HOME/bin:$PATH</code></pre>

<hr>

<h3>Verify Installation</h3>
<pre><code># In terminal/command prompt
spark-submit --version

# Start Jupyter Lab
jupyter lab</code></pre>

<h2 id="part2">Part 2: PySpark Fundamentals</h2>

<h3>Initialize Spark Session</h3>
<pre><code>from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
from pyspark.sql.window import Window

# Local development session
spark = SparkSession.builder \
    .appName("LocalDev") \
    .master("local[*]") \
    .config("spark.driver.memory", "4g") \
    .config("spark.sql.shuffle.partitions", "8") \
    .getOrCreate()

# Access Spark UI at http://localhost:4040</code></pre>

<h3>Creating DataFrames</h3>
<pre><code># From Python list
data = [(&quot;Alice&quot;, 34, &quot;NYC&quot;), (&quot;Bob&quot;, 45, &quot;LA&quot;)]
df = spark.createDataFrame(data, [&quot;name&quot;, &quot;age&quot;, &quot;city&quot;])

# ---- Sample datasets (from Archive.zip) ----
# 1) Extract Archive.zip next to your notebook as: data/
DATA_DIR = &quot;data&quot;  # &quot;./data&quot; locally, or &quot;/dbfs/FileStore/data&quot; in Databricks

customers = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/customers.csv&quot;))

sales = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/sales.csv&quot;))

products = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/products.csv&quot;))

departments = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/departments.csv&quot;))

employees = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/employees.csv&quot;))

web_logs = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/web_logs.csv&quot;))

skewed = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/skewed_data.csv&quot;))

ml = (spark.read.option(&quot;header&quot;, True).option(&quot;inferSchema&quot;, True)
    .csv(f&quot;{DATA_DIR}/ml_features.csv&quot;))

# JSON (newline-delimited JSON)
users_json = spark.read.json(f&quot;{DATA_DIR}/sample_data.json&quot;)

# Parquet (same datasets are included as parquet too)
customers_pq = spark.read.parquet(f&quot;{DATA_DIR}/customers.parquet&quot;)</code></pre>

<h3>DataFrame Inspection</h3>
<pre><code>df.show(10)                    # Display first 10 rows
df.show(truncate=False)        # Show full content
df.printSchema()               # Show schema with data types
df.dtypes                      # List of (column, type) tuples
df.columns                     # List of column names
df.count()                     # Number of rows
df.describe().show()           # Summary statistics
df.explain()                   # Show execution plan
df.explain(True)               # Detailed execution plan</code></pre>

<h3>Column Selection &amp; Manipulation</h3>
<pre><code># Select columns
df.select("name", "age")
df.select(F.col("name"), F.col("age"))
df.select(df["name"], df.age)

# Select with expressions
df.select(
    F.col("name"),
    (F.col("age") + 10).alias("age_plus_10"),
    F.upper(F.col("name")).alias("name_upper")
)

# Add new columns
df.withColumn("new_col", F.lit("constant"))
df.withColumn("age_doubled", F.col("age") * 2)

# Rename columns
df.withColumnRenamed("old_name", "new_name")

# Drop columns
df.drop("column1", "column2")</code></pre>

<h2 id="part3">Part 3: Common Transformations</h2>

<h3>Filtering Data</h3>
<pre><code># Basic filters
df.filter(F.col("age") > 30)
df.filter("age > 30")                    # SQL expression
df.where(F.col("city") == "NYC")

# Multiple conditions
df.filter((F.col("age") > 30) & (F.col("city") == "NYC"))
df.filter((F.col("age") < 25) | (F.col("age") > 60))

# IN clause
df.filter(F.col("city").isin(["NYC", "LA", "Chicago"]))

# NULL handling
df.filter(F.col("name").isNotNull())
df.filter(F.col("name").isNull())

# String matching
df.filter(F.col("name").like("%Alice%"))
df.filter(F.col("name").rlike("^A.*"))  # Regex</code></pre>

<h3>Aggregations</h3>
<pre><code># Basic aggregations
df.groupBy("city").count()
df.groupBy("city").agg(
    F.count("*").alias("total"),
    F.avg("age").alias("avg_age"),
    F.max("age").alias("max_age"),
    F.min("age").alias("min_age"),
    F.sum("salary").alias("total_salary"),
    F.countDistinct("department").alias("unique_depts")
)

# Multiple grouping columns
df.groupBy("city", "department").agg(...)

# Collect values into list
df.groupBy("city").agg(
    F.collect_list("name").alias("names"),
    F.collect_set("department").alias("unique_depts")
)</code></pre>

<h3>Joins</h3>
<pre><code># Sample dataset example
# sales enriched with customer + product + sales rep attributes
sales_enriched = (sales
    .join(customers.select(&quot;customer_id&quot;, &quot;name&quot;, &quot;city&quot;, &quot;state&quot;, &quot;tier&quot;), on=&quot;customer_id&quot;, how=&quot;left&quot;)
    .join(products.select(&quot;product&quot;, &quot;category&quot;, &quot;msrp&quot;, &quot;cost&quot;), on=&quot;product&quot;, how=&quot;left&quot;)
    .join(employees.select(F.col(&quot;id&quot;).alias(&quot;sales_rep_id&quot;), &quot;first_name&quot;, &quot;last_name&quot;, &quot;department&quot;),
          on=&quot;sales_rep_id&quot;, how=&quot;left&quot;)
)

# Anti-join: customers with no purchases
customers_no_sales = customers.join(sales.select(&quot;customer_id&quot;).distinct(), on=&quot;customer_id&quot;, how=&quot;left_anti&quot;)

# Quick reference for join types
df1.join(df2, on=&quot;id&quot;, how=&quot;inner&quot;)   # inner (default)
df1.join(df2, on=&quot;id&quot;, how=&quot;left&quot;)    # left
df1.join(df2, on=&quot;id&quot;, how=&quot;right&quot;)   # right
df1.join(df2, on=&quot;id&quot;, how=&quot;outer&quot;)   # full outer
df1.join(df2, on=&quot;id&quot;, how=&quot;left_semi&quot;)
df1.join(df2, on=&quot;id&quot;, how=&quot;left_anti&quot;)</code></pre>

<h3>Window Functions</h3>
<pre><code>from pyspark.sql.window import Window

# Define window specification
window_spec = Window.partitionBy("department").orderBy(F.desc("salary"))

# Ranking functions
df.withColumn("rank", F.rank().over(window_spec))
df.withColumn("dense_rank", F.dense_rank().over(window_spec))
df.withColumn("row_number", F.row_number().over(window_spec))
df.withColumn("ntile", F.ntile(4).over(window_spec))  # Quartiles

# Analytic functions
df.withColumn("prev_salary", F.lag("salary", 1).over(window_spec))
df.withColumn("next_salary", F.lead("salary", 1).over(window_spec))
df.withColumn("first_val", F.first("salary").over(window_spec))
df.withColumn("last_val", F.last("salary").over(window_spec))

# Running totals
running_window = Window.partitionBy("dept").orderBy("date") \
    .rowsBetween(Window.unboundedPreceding, Window.currentRow)
df.withColumn("running_total", F.sum("amount").over(running_window))</code></pre>

<h2 id="part4">Part 4: String &amp; Date Functions</h2>

<h3>String Functions</h3>
<pre><code># Case conversion (sample: customers)
customers.select(F.upper(&quot;name&quot;), F.lower(&quot;name&quot;), F.initcap(&quot;name&quot;)).show(5, truncate=False)

# Trim and padding
customers.select(
    F.trim(&quot;name&quot;).alias(&quot;name_trim&quot;),
    F.lpad(F.col(&quot;customer_id&quot;).cast(&quot;string&quot;), 8, &quot;0&quot;).alias(&quot;customer_id_padded&quot;)
).show(5, truncate=False)

# Substring and length
customers.select(
    &quot;name&quot;,
    F.substring(&quot;name&quot;, 1, 3).alias(&quot;name_prefix&quot;),
    F.length(&quot;name&quot;).alias(&quot;name_len&quot;)
).show(5, truncate=False)

# Split and explode (customers.tags is a comma-separated string)
customers.select(F.split(&quot;tags&quot;, &quot;,&quot;).alias(&quot;tag_array&quot;)).show(5, truncate=False)
customers.select(F.explode(F.split(&quot;tags&quot;, &quot;,&quot;)).alias(&quot;tag&quot;)).groupBy(&quot;tag&quot;).count().show()

# Regex examples
customers.select(
    F.regexp_extract(&quot;email&quot;, r&quot;(.+)@(.+)&quot;, 1).alias(&quot;email_user&quot;),
    F.regexp_replace(&quot;phone&quot;, r&quot;[^0-9]&quot;, &quot;&quot;).alias(&quot;clean_phone&quot;)
).show(5, truncate=False)

web_logs.select(
    &quot;page&quot;,
    F.regexp_extract(&quot;page&quot;, r&quot;^/([^/]+)&quot;, 1).alias(&quot;route&quot;)
).show(5, truncate=False)</code></pre>

<h3>Date &amp; Timestamp Functions</h3>
<pre><code># Current date/time
spark.range(1).select(F.current_date().alias(&quot;today&quot;), F.current_timestamp().alias(&quot;now&quot;)).show()

# Parse strings to dates/timestamps (sample datasets)
sales2 = sales.withColumn(&quot;sale_date&quot;, F.to_date(&quot;date&quot;, &quot;yyyy-MM-dd&quot;))
logs2 = web_logs.withColumn(&quot;ts&quot;, F.to_timestamp(&quot;timestamp&quot;))

# Extract components
sales2.select(
    &quot;sale_id&quot;,
    &quot;sale_date&quot;,
    F.year(&quot;sale_date&quot;).alias(&quot;year&quot;),
    F.month(&quot;sale_date&quot;).alias(&quot;month&quot;),
    F.dayofmonth(&quot;sale_date&quot;).alias(&quot;day&quot;)
).show(5)

logs2.select(
    &quot;log_id&quot;,
    &quot;ts&quot;,
    F.hour(&quot;ts&quot;).alias(&quot;hour&quot;),
    F.minute(&quot;ts&quot;).alias(&quot;minute&quot;)
).show(5)

# Date arithmetic
sales2.select(
    &quot;sale_date&quot;,
    F.date_add(&quot;sale_date&quot;, 7).alias(&quot;plus_7d&quot;),
    F.date_sub(&quot;sale_date&quot;, 30).alias(&quot;minus_30d&quot;)
).show(5)

# Format dates
sales2.select(F.date_format(&quot;sale_date&quot;, &quot;MMM dd, yyyy&quot;).alias(&quot;pretty_date&quot;)).show(5, truncate=False)</code></pre>

<h2 id="part5">Part 5: Data Engineering Patterns</h2>

<h3>Handling Nulls &amp; Duplicates</h3>
<pre><code># Fill nulls
df.na.fill(0)                          # Fill all nulls with 0
df.na.fill({"age": 0, "name": "Unknown"})
df.fillna({"salary": df.agg(F.avg("salary")).first()[0]})

# Drop nulls
df.na.drop()                           # Drop rows with any null
df.na.drop("all")                      # Drop only if all values null
df.na.drop(subset=["name", "age"])     # Check specific columns

# Coalesce (return first non-null)
df.select(F.coalesce("preferred_name", "name").alias("display_name"))

# Remove duplicates
df.dropDuplicates()                    # All columns
df.dropDuplicates(["email"])           # Based on specific columns

# Keep first/last duplicate based on ordering
window = Window.partitionBy("email").orderBy(F.desc("created_at"))
df.withColumn("rn", F.row_number().over(window)) \
  .filter(F.col("rn") == 1).drop("rn")</code></pre>

<h3>Schema Definition &amp; Enforcement</h3>
<pre><code>from pyspark.sql.types import *

# Define explicit schema
schema = StructType([
    StructField("id", LongType(), nullable=False),
    StructField("name", StringType(), nullable=True),
    StructField("age", IntegerType(), nullable=True),
    StructField("salary", DoubleType(), nullable=True),
    StructField("hire_date", DateType(), nullable=True),
    StructField("metadata", MapType(StringType(), StringType())),
    StructField("tags", ArrayType(StringType())),
    StructField("address", StructType([
        StructField("city", StringType()),
        StructField("zip", StringType())
    ]))
])

# Read with schema (faster than inferSchema)
df = spark.read.schema(schema).json("data.json")

# Cast column types
df.withColumn("age", F.col("age").cast(IntegerType()))
df.withColumn("amount", F.col("amount").cast("decimal(10,2)"))</code></pre>

<h3>Writing Data</h3>
<pre><code># Write to Parquet (recommended)
df.write.mode("overwrite").parquet("output/data.parquet")

# Write modes: overwrite, append, ignore, error (default)
df.write.mode("append").parquet("output/")

# Partitioned write (critical for large datasets)
df.write \
    .mode("overwrite") \
    .partitionBy("year", "month") \
    .parquet("output/partitioned/")

# Control number of output files
df.coalesce(1).write.csv("single_file/")   # Single file
df.repartition(10).write.parquet("output/") # 10 files

# Write to CSV with options
df.write \
    .mode("overwrite") \
    .option("header", True) \
    .option("delimiter", ",") \
    .csv("output.csv")

# Write to Delta (if using Delta Lake)
df.write.format("delta").mode("overwrite").save("delta_table/")</code></pre>

<h2 id="part6">Part 6: PySpark ML for Data Scientists</h2>

<h3>Feature Engineering</h3>
<pre><code>from pyspark.ml.feature import (
    VectorAssembler, StandardScaler, StringIndexer,
    OneHotEncoder, Bucketizer, Imputer
)

# Combine features into vector
assembler = VectorAssembler(
    inputCols=["age", "salary", "experience"],
    outputCol="features"
)
df = assembler.transform(df)

# Scale features
scaler = StandardScaler(
    inputCol="features",
    outputCol="scaled_features",
    withMean=True, withStd=True
)
scaler_model = scaler.fit(df)
df = scaler_model.transform(df)

# Encode categorical variables
indexer = StringIndexer(inputCol="category", outputCol="category_idx")
encoder = OneHotEncoder(inputCol="category_idx", outputCol="category_vec")

# Handle missing values
imputer = Imputer(
    inputCols=["age", "salary"],
    outputCols=["age_imputed", "salary_imputed"],
    strategy="median"
)</code></pre>

<h3>ML Pipeline Example</h3>
<pre><code>from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler

# Sample dataset: ml_features.csv (loaded as `ml`)
df = ml.dropna(subset=[&quot;label&quot;])

train, test = df.randomSplit([0.8, 0.2], seed=42)

cat_cols = [&quot;category&quot;, &quot;region&quot;]
num_cols = [&quot;age&quot;, &quot;income&quot;, &quot;credit_score&quot;, &quot;years_employed&quot;, &quot;num_accounts&quot;]

indexers = [StringIndexer(inputCol=c, outputCol=f&quot;{c}_idx&quot;, handleInvalid=&quot;keep&quot;) for c in cat_cols]
encoders = [OneHotEncoder(inputCol=f&quot;{c}_idx&quot;, outputCol=f&quot;{c}_vec&quot;) for c in cat_cols]

assembler = VectorAssembler(
    inputCols=num_cols + [f&quot;{c}_vec&quot; for c in cat_cols],
    outputCol=&quot;features&quot;
)

rf = RandomForestClassifier(labelCol=&quot;label&quot;, featuresCol=&quot;features&quot;, numTrees=200, seed=42)

pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])

model = pipeline.fit(train)
pred = model.transform(test)

auc = BinaryClassificationEvaluator(labelCol=&quot;label&quot;).evaluate(pred)
print(&quot;AUC:&quot;, auc)</code></pre>

<h3>Pandas UDFs for Advanced Analytics</h3>
<pre><code>import pandas as pd
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import DoubleType

# Scalar UDF - applied row by row (vectorized)
@pandas_udf(DoubleType())
def normalize(series: pd.Series) -> pd.Series:
    return (series - series.mean()) / series.std()

df.select(normalize("salary").alias("normalized_salary"))

# Grouped Map UDF - apply function to each group
@pandas_udf(df.schema, functionType=PandasUDFType.GROUPED_MAP)
def subtract_mean(pdf: pd.DataFrame) -> pd.DataFrame:
    pdf["salary"] = pdf["salary"] - pdf["salary"].mean()
    return pdf

df.groupby("department").apply(subtract_mean)</code></pre>

<h2 id="part7">Part 7: Performance Optimization</h2>

<h3>Caching &amp; Persistence</h3>
<pre><code>from pyspark import StorageLevel

# Cache in memory (use when DF is reused multiple times)
df.cache()            # Same as persist(StorageLevel.MEMORY_ONLY)
df.persist(StorageLevel.MEMORY_AND_DISK)

# Unpersist when done
df.unpersist()

# Check if cached
df.is_cached

# Force evaluation and cache
df.cache().count()    # Triggers computation and caches</code></pre>

<h3>Partitioning Strategies</h3>
<pre><code># Check current partitions
df.rdd.getNumPartitions()

# Repartition (shuffle - expensive but even distribution)
df.repartition(200)                    # By number
df.repartition("key_column")           # By column (hash)
df.repartition(200, "key_column")      # Both

# Coalesce (no shuffle - for reducing partitions)
df.coalesce(10)                        # Combine into fewer

# Random repartition (for skew issues)
df.withColumn("salt", F.rand()) \
  .repartition(200, "salt") \
  .drop("salt")

# Partition size recommendation: 128 MB per partition
# Calculate: total_data_size_MB / 128 = num_partitions</code></pre>

<h3>Broadcast Joins</h3>
<pre><code>from pyspark.sql.functions import broadcast

# Force broadcast (small table joins large table)
# Best when small table < 10MB (configurable)
result = large_df.join(
    broadcast(small_df),
    "join_key"
)

# Configure auto broadcast threshold (default 10MB)
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 100*1024*1024)  # 100MB</code></pre>

<h3>Key Configuration Parameters</h3>
<table>
    <tr>
        <th>Parameter</th>
        <th>Default</th>
        <th>Recommendation</th>
    </tr>
    <tr>
        <td><code>spark.sql.shuffle.partitions</code></td>
        <td>200</td>
        <td>Set to data_size_GB √ó 4 (target 128MB/partition)</td>
    </tr>
    <tr>
        <td><code>spark.default.parallelism</code></td>
        <td>Total cores</td>
        <td>2-3x total cores for better parallelism</td>
    </tr>
    <tr>
        <td><code>spark.sql.adaptive.enabled</code></td>
        <td>true (3.0+)</td>
        <td>Enable for automatic optimization</td>
    </tr>
    <tr>
        <td><code>spark.executor.memory</code></td>
        <td>1g</td>
        <td>4-8g per executor typically</td>
    </tr>
    <tr>
        <td><code>spark.executor.cores</code></td>
        <td>1</td>
        <td>4-5 cores per executor is optimal</td>
    </tr>
    <tr>
        <td><code>spark.memory.fraction</code></td>
        <td>0.6</td>
        <td>Increase to 0.8 for memory-heavy jobs</td>
    </tr>
</table>

<h2 id="part8">Part 8: EMR Job Troubleshooting Guide</h2>

<p><em>Common issues encountered when running PySpark jobs on AWS EMR and their solutions.</em></p>

<h3>Issue 1: Data Skew - Uneven Partition Sizes</h3>

<div class="symptom">
    <strong>Symptom:</strong> One or few tasks take much longer than others. Some executors are idle while others are overloaded. Job appears stuck at 99% for a long time.
</div>

<h4>Diagnosis</h4>
<pre><code># Sample dataset: skewed_data.csv (loaded as `skewed`)
from pyspark.sql.functions import spark_partition_id

# Check partition sizes
(skewed.groupBy(spark_partition_id().alias(&quot;partition_id&quot;))
  .count()
  .orderBy(F.desc(&quot;count&quot;))
  .show(50))

# Check for skewed keys
(skewed.groupBy(&quot;join_key&quot;).count()
  .orderBy(F.desc(&quot;count&quot;))
  .show(20))</code></pre>

<h4>Solution: Salted Repartition</h4>
<pre><code># Add random salt and repartition evenly
from pyspark.sql.functions import rand, floor

num_partitions = 500  # Adjust based on data size

df_rebalanced = df \
    .withColumn("_salt", floor(rand() * num_partitions).cast("int")) \
    .repartition(num_partitions, "_salt") \
    .drop("_salt")

# For skewed joins, salt both sides
salt_buckets = 10

# Salt the large/skewed table
df_large_salted = df_large \
    .withColumn("_salt", floor(rand() * salt_buckets).cast("int"))

# Explode the small table to match all salts
df_small_exploded = df_small \
    .withColumn("_salt", F.explode(F.array([F.lit(i) for i in range(salt_buckets)])))

# Join on original key + salt
result = df_large_salted.join(
    df_small_exploded,
    ["join_key", "_salt"],
    "inner"
).drop("_salt")</code></pre>

<hr>

<h3>Issue 2: Jobs Taking Too Long - Insufficient Parallelism</h3>

<div class="symptom">
    <strong>Symptom:</strong> Jobs run slowly even with large clusters. Spark UI shows few active tasks. Shuffle operations are bottlenecked.
</div>

<h4>Root Cause</h4>
<p>Default shuffle partitions (200) is too low for large datasets, resulting in partitions that are too large (> 200MB each).</p>

<h4>Solution: Increase Shuffle Partitions</h4>
<pre><code># Calculate optimal partitions: target 128MB per partition
# For 100GB dataset: 100 * 1024 / 128 = 800 partitions

# Set at session level
spark.conf.set("spark.sql.shuffle.partitions", 800)

# Or in spark-submit
spark-submit \
    --conf spark.sql.shuffle.partitions=800 \
    --conf spark.default.parallelism=800 \
    your_job.py

# Enable Adaptive Query Execution (Spark 3.0+)
spark.conf.set("spark.sql.adaptive.enabled", True)
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", True)
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", True)</code></pre>

<hr>

<h3>Issue 3: Out of Memory Errors</h3>

<div class="symptom">
    <strong>Symptom:</strong> java.lang.OutOfMemoryError, Container killed by YARN for exceeding memory limits, GC overhead limit exceeded.
</div>

<h4>Solutions</h4>
<pre><code># 1. Increase executor memory and overhead
spark-submit \
    --executor-memory 8g \
    --conf spark.executor.memoryOverhead=2g \
    --conf spark.memory.fraction=0.8 \
    your_job.py

# 2. Reduce executor cores (less concurrent tasks per executor)
--executor-cores 4  # Instead of 5

# 3. Increase partitions to reduce per-partition memory
spark.conf.set("spark.sql.shuffle.partitions", 1000)

# 4. Avoid collect() on large datasets
# BAD:
all_data = df.collect()  # Brings all data to driver!

# GOOD:
df.write.parquet("output/")  # Write to storage instead

# 5. Use approximate distinct instead of exact
df.select(F.approx_count_distinct("user_id"))  # Instead of countDistinct</code></pre>

<hr>

<h3>Issue 4: Slow Job Startup / Long Scheduling Delays</h3>

<div class="symptom">
    <strong>Symptom:</strong> Jobs pending for long time before starting. Spark UI shows scheduling delay.
</div>

<h4>Solutions</h4>
<pre><code># 1. Reduce number of small files (small files problem)
# Compact input files first
df = spark.read.parquet("input/")
df.coalesce(100).write.parquet("compacted/")

# 2. Increase YARN resources
# In EMR cluster configuration:
# yarn.nodemanager.resource.memory-mb: 60000
# yarn.scheduler.maximum-allocation-mb: 60000

# 3. Use fewer, larger executors
--num-executors 20 \
--executor-memory 16g \
--executor-cores 5

# 4. Enable dynamic allocation
--conf spark.dynamicAllocation.enabled=true \
--conf spark.dynamicAllocation.minExecutors=10 \
--conf spark.dynamicAllocation.maxExecutors=100</code></pre>

<hr>

<h3>Issue 5: Shuffle Spill to Disk</h3>

<div class="symptom">
    <strong>Symptom:</strong> Excessive shuffle read/write. Jobs slow due to disk I/O. Spark UI shows 'Shuffle Spill (Disk)'.
</div>

<h4>Solutions</h4>
<pre><code># 1. Increase memory for shuffle
spark.conf.set("spark.memory.fraction", 0.8)  # Default 0.6
spark.conf.set("spark.memory.storageFraction", 0.3)  # Default 0.5

# 2. Reduce shuffle data by filtering early
# BAD: Filter after join
result = df1.join(df2, "key").filter("date > '2024-01-01'")

# GOOD: Filter before join
df1_filtered = df1.filter("date > '2024-01-01'")
result = df1_filtered.join(df2, "key")

# 3. Use broadcast for small dimension tables
from pyspark.sql.functions import broadcast
result = large_df.join(broadcast(small_dim), "key")

# 4. Select only needed columns before shuffle
df.select("key", "needed_col1", "needed_col2") \
  .groupBy("key").agg(...)</code></pre>

<hr>

<h3>EMR Best Practices Summary</h3>
<table>
    <tr>
        <th>Scenario</th>
        <th>Configuration</th>
    </tr>
    <tr>
        <td>General ETL</td>
        <td>shuffle.partitions = data_size_GB √ó 4, executor-memory 8g, executor-cores 5</td>
    </tr>
    <tr>
        <td>Skewed Joins</td>
        <td>Enable AQE: adaptive.skewJoin.enabled = true, or use salting technique</td>
    </tr>
    <tr>
        <td>Many Small Files</td>
        <td>Pre-compact to 128-256MB files, use coalesce() in writes</td>
    </tr>
    <tr>
        <td>Large Aggregations</td>
        <td>Increase shuffle.partitions, use 2-stage aggregation for high cardinality</td>
    </tr>
    <tr>
        <td>Memory Issues</td>
        <td>Reduce executor-cores to 4, increase memoryOverhead, avoid collect()</td>
    </tr>
</table>

<h2 id="part9">Part 9: Quick Reference Card</h2>

<h3>Common Import Template</h3>
<pre><code>from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
from pyspark.sql.window import Window
from pyspark.sql.functions import (
    col, lit, when, coalesce, concat, concat_ws,
    sum, avg, count, max, min, countDistinct,
    year, month, dayofmonth, date_format, to_date,
    split, explode, array, struct, collect_list,
    row_number, rank, dense_rank, lead, lag,
    broadcast, spark_partition_id, rand
)</code></pre>

<h3>Spark UI Tabs Reference</h3>
<table>
    <tr>
        <th>Tab</th>
        <th>What to Look For</th>
    </tr>
    <tr>
        <td>Jobs</td>
        <td>Failed jobs, skipped stages (good - means data was cached)</td>
    </tr>
    <tr>
        <td>Stages</td>
        <td>Task duration variance, shuffle read/write sizes, skew indicators</td>
    </tr>
    <tr>
        <td>Storage</td>
        <td>Cached DataFrames, memory usage per partition</td>
    </tr>
    <tr>
        <td>Executors</td>
        <td>GC time (>10% is bad), shuffle spill, failed tasks</td>
    </tr>
    <tr>
        <td>SQL</td>
        <td>Query plans, join strategies (broadcast vs shuffle), scan sizes</td>
    </tr>
</table>

<h3>Useful Spark Shell Commands</h3>
<pre><code># Start PySpark shell with custom config
pyspark --master local[4] \
        --driver-memory 4g \
        --conf spark.sql.shuffle.partitions=8

# Start with Jupyter notebook
PYSPARK_DRIVER_PYTHON=jupyter \
PYSPARK_DRIVER_PYTHON_OPTS='notebook' \
pyspark

# Submit job to EMR
spark-submit \
    --master yarn \
    --deploy-mode cluster \
    --num-executors 20 \
    --executor-memory 8g \
    --executor-cores 5 \
    --conf spark.sql.shuffle.partitions=500 \
    --conf spark.sql.adaptive.enabled=true \
    s3://bucket/scripts/my_job.py</code></pre>

<div class="tip">
    <strong>üí° Pro Tip:</strong> Always check the Spark UI (port 4040) to understand your job's behavior. The SQL tab shows execution plans that reveal exactly how Spark processes your queries.
</div>

<h2 id="part10">Part 10: Core Concepts &amp; Interview Questions</h2>

<h3>Spark Architecture Fundamentals</h3>

<h4>Q: What is Apache Spark and how does it differ from MapReduce?</h4>
<p>Spark is a distributed computing engine for large-scale data processing. Key differences from MapReduce:</p>
<table>
    <tr><th>Feature</th><th>MapReduce</th><th>Spark</th></tr>
    <tr><td>Processing</td><td>Disk-based between stages</td><td>In-memory (up to 100x faster)</td></tr>
    <tr><td>Programming Model</td><td>Map &rarr; Reduce only</td><td>Map, Reduce, Join, Window, ML, Graph, Streaming</td></tr>
    <tr><td>Languages</td><td>Java</td><td>Python, Scala, Java, R, SQL</td></tr>
    <tr><td>Iterative Jobs</td><td>Writes to disk each iteration</td><td>Keeps data in memory across iterations</td></tr>
    <tr><td>Real-time</td><td>Batch only</td><td>Batch + Streaming (micro-batch &amp; continuous)</td></tr>
</table>

<h4>Q: Explain Spark's cluster architecture.</h4>
<pre><code>Driver Program (SparkContext / SparkSession)
    ‚îÇ
    ‚îú‚îÄ‚îÄ Cluster Manager (YARN / Mesos / K8s / Standalone)
    ‚îÇ
    ‚îú‚îÄ‚îÄ Executor 1 (Worker Node)
    ‚îÇ   ‚îú‚îÄ‚îÄ Task 1  ‚îÄ‚îÄ Partition 1
    ‚îÇ   ‚îú‚îÄ‚îÄ Task 2  ‚îÄ‚îÄ Partition 2
    ‚îÇ   ‚îî‚îÄ‚îÄ Cache (Block Manager)
    ‚îÇ
    ‚îî‚îÄ‚îÄ Executor 2 (Worker Node)
        ‚îú‚îÄ‚îÄ Task 3  ‚îÄ‚îÄ Partition 3
        ‚îú‚îÄ‚îÄ Task 4  ‚îÄ‚îÄ Partition 4
        ‚îî‚îÄ‚îÄ Cache (Block Manager)</code></pre>
<ul>
    <li><strong>Driver</strong>: Creates SparkContext, builds the DAG, schedules tasks, collects results</li>
    <li><strong>Cluster Manager</strong>: Allocates resources across the cluster</li>
    <li><strong>Executors</strong>: JVM processes on worker nodes that run tasks and cache data</li>
    <li><strong>Tasks</strong>: Smallest unit of work, each processing one partition</li>
</ul>

<hr>

<h3>RDD vs DataFrame vs Dataset</h3>

<h4>Q: What are the differences between RDD, DataFrame, and Dataset?</h4>
<table>
    <tr><th>Feature</th><th>RDD</th><th>DataFrame</th><th>Dataset</th></tr>
    <tr><td>Type Safety</td><td>Compile-time</td><td>Runtime</td><td>Compile-time (Scala/Java only)</td></tr>
    <tr><td>Optimization</td><td>No Catalyst/Tungsten</td><td>Full Catalyst + Tungsten</td><td>Full Catalyst + Tungsten</td></tr>
    <tr><td>Schema</td><td>No schema</td><td>Schema (StructType)</td><td>Schema (case class / Encoder)</td></tr>
    <tr><td>API</td><td>Functional (map, filter)</td><td>Declarative (select, where)</td><td>Both</td></tr>
    <tr><td>Serialization</td><td>Java serialization</td><td>Tungsten binary (off-heap)</td><td>Tungsten binary</td></tr>
    <tr><td>Use Case</td><td>Low-level control, unstructured data</td><td>Structured/semi-structured data</td><td>Type-safe structured (Scala)</td></tr>
    <tr><td>Python Support</td><td>Yes</td><td>Yes</td><td>No (Scala/Java only)</td></tr>
</table>

<pre><code># RDD example (low-level, avoid in modern PySpark)
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
rdd.map(lambda x: x * 2).filter(lambda x: x &gt; 4).collect()  # [6, 8, 10]

# DataFrame example (preferred in PySpark)
df = spark.createDataFrame([(1,), (2,), (3,), (4,), (5,)], [&quot;num&quot;])
df.withColumn(&quot;doubled&quot;, F.col(&quot;num&quot;) * 2).filter(F.col(&quot;doubled&quot;) &gt; 4).show()

# Convert between RDD and DataFrame
rdd_from_df = df.rdd                            # DataFrame &rarr; RDD
df_from_rdd = rdd.toDF([&quot;value&quot;])               # RDD &rarr; DataFrame</code></pre>

<hr>

<h3>Lazy Evaluation &amp; DAG</h3>

<h4>Q: What is lazy evaluation in Spark? Why is it important?</h4>
<p>Spark uses <strong>lazy evaluation</strong> &mdash; transformations are not executed immediately. Instead, Spark builds a <strong>DAG (Directed Acyclic Graph)</strong> of transformations. Execution only happens when an <strong>action</strong> is called.</p>

<p><strong>Transformations</strong> (lazy &mdash; return a new DataFrame):</p>
<ul>
    <li><code>select()</code>, <code>filter()</code>, <code>groupBy()</code>, <code>join()</code>, <code>withColumn()</code>, <code>orderBy()</code></li>
    <li><code>map()</code>, <code>flatMap()</code>, <code>union()</code>, <code>distinct()</code>, <code>repartition()</code></li>
</ul>

<p><strong>Actions</strong> (trigger execution &mdash; return results):</p>
<ul>
    <li><code>show()</code>, <code>count()</code>, <code>collect()</code>, <code>take()</code>, <code>first()</code></li>
    <li><code>write.*</code>, <code>foreach()</code>, <code>reduce()</code>, <code>toPandas()</code></li>
</ul>

<pre><code># Nothing executes yet &mdash; Spark just records the plan
df_filtered = df.filter(F.col(&quot;age&quot;) &gt; 30)          # Transformation
df_selected = df_filtered.select(&quot;name&quot;, &quot;salary&quot;)   # Transformation
df_sorted = df_selected.orderBy(F.desc(&quot;salary&quot;))    # Transformation

# NOW Spark executes the entire chain (optimized by Catalyst)
df_sorted.show(10)  # Action &mdash; triggers execution

# View the execution plan
df_sorted.explain(True)</code></pre>

<div class="tip">
    <strong>Benefits of lazy evaluation:</strong>
    <ol>
        <li><strong>Optimization</strong>: Catalyst can optimize the entire chain (predicate pushdown, column pruning)</li>
        <li><strong>Efficiency</strong>: Avoids unnecessary intermediate computation</li>
        <li><strong>Pipelining</strong>: Multiple transformations are combined into single-stage tasks</li>
    </ol>
</div>

<hr>

<h3>Narrow vs Wide Transformations</h3>

<h4>Q: What is the difference between narrow and wide transformations?</h4>
<table>
    <tr><th>Narrow Transformations</th><th>Wide Transformations</th></tr>
    <tr><td>Each input partition contributes to at most one output partition</td><td>Input partitions contribute to multiple output partitions</td></tr>
    <tr><td>No data shuffle across the network</td><td>Requires shuffle (data exchange between executors)</td></tr>
    <tr><td><code>map</code>, <code>filter</code>, <code>select</code>, <code>withColumn</code>, <code>union</code></td><td><code>groupBy</code>, <code>join</code>, <code>orderBy</code>, <code>repartition</code>, <code>distinct</code></td></tr>
    <tr><td>Fast, pipelined within a stage</td><td>Expensive, creates a new stage (stage boundary)</td></tr>
</table>

<hr>

<h3>Catalyst Optimizer &amp; Tungsten</h3>

<h4>Q: What is the Catalyst Optimizer?</h4>
<p>Catalyst is Spark SQL's query optimizer that transforms logical plans into optimized physical plans:</p>
<ol>
    <li><strong>Analysis</strong>: Resolves column names, types, tables using the catalog</li>
    <li><strong>Logical Optimization</strong>: Applies rule-based optimizations
        <ul>
            <li><strong>Predicate pushdown</strong>: Push filters closer to data source</li>
            <li><strong>Column pruning</strong>: Read only needed columns</li>
            <li><strong>Constant folding</strong>: Pre-compute constant expressions</li>
            <li><strong>Boolean simplification</strong>: Simplify boolean expressions</li>
        </ul>
    </li>
    <li><strong>Physical Planning</strong>: Choose join strategies (broadcast vs sort-merge), scan methods</li>
    <li><strong>Code Generation (Tungsten)</strong>: Generate optimized Java bytecode at runtime</li>
</ol>

<pre><code># Catalyst automatically optimizes this:
df.select(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;, &quot;department&quot;) \
  .filter(F.col(&quot;age&quot;) &gt; 30) \
  .groupBy(&quot;department&quot;) \
  .agg(F.avg(&quot;salary&quot;))

# Catalyst pushes filter BEFORE the select, reads only 4 columns
# even if the source has 50 columns (column pruning)

# See the optimized plan:
df.filter(F.col(&quot;age&quot;) &gt; 30).select(&quot;name&quot;).explain(True)</code></pre>

<h4>What is Tungsten?</h4>
<ul>
    <li><strong>Off-heap memory</strong>: Bypasses JVM garbage collection</li>
    <li><strong>Cache-aware computation</strong>: Optimizes CPU cache usage</li>
    <li><strong>Whole-stage code generation</strong>: Fuses operators into single Java function</li>
    <li><strong>Binary encoding</strong>: Compact representation, avoids Java object overhead</li>
</ul>

<hr>

<h3>Shuffle Deep Dive</h3>

<h4>Q: What is a shuffle and why is it expensive?</h4>
<p>A shuffle redistributes data across partitions (and executors) over the network. It is the most expensive operation in Spark.</p>

<pre><code>Stage 1 (Map side)                    Stage 2 (Reduce side)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Partition 1     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ shuffle ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Partition A     ‚îÇ
‚îÇ (key: a, b, c)  ‚îÇ    write/read     ‚îÇ (all key=a)     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Partition 2     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ shuffle ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Partition B     ‚îÇ
‚îÇ (key: a, c, d)  ‚îÇ                   ‚îÇ (all key=b)     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Partition 3     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ shuffle ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Partition C     ‚îÇ
‚îÇ (key: b, d, e)  ‚îÇ                   ‚îÇ (all key=c,d,e) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>

<p><strong>How to minimize shuffles:</strong></p>
<pre><code># 1. Use broadcast joins for small tables
result = large_df.join(broadcast(small_df), &quot;key&quot;)

# 2. Filter early to reduce shuffle data
df_filtered = df.filter(F.col(&quot;date&quot;) &gt; &quot;2024-01-01&quot;)
df_filtered.groupBy(&quot;key&quot;).count()

# 3. Use coalesce instead of repartition when reducing partitions
df.coalesce(10)  # No shuffle vs df.repartition(10) which shuffles

# 4. Pre-partition data on join keys
df.write.partitionBy(&quot;join_key&quot;).parquet(&quot;output/&quot;)</code></pre>

<hr>

<h3>Spark Memory Management</h3>

<h4>Q: Explain Spark's memory model.</h4>
<pre><code>Executor Memory (spark.executor.memory, e.g., 8g)
‚îú‚îÄ‚îÄ Reserved Memory (300MB fixed)
‚îú‚îÄ‚îÄ User Memory (1 - spark.memory.fraction) √ó (Total - 300MB)
‚îÇ   ‚îî‚îÄ‚îÄ User data structures, UDF variables, metadata
‚îî‚îÄ‚îÄ Unified Memory (spark.memory.fraction, default 0.6) √ó (Total - 300MB)
    ‚îú‚îÄ‚îÄ Storage Memory (for cache/persist)
    ‚îÇ   ‚îî‚îÄ‚îÄ Can borrow from Execution if available
    ‚îî‚îÄ‚îÄ Execution Memory (for shuffles, sorts, joins, aggregations)
        ‚îî‚îÄ‚îÄ Can evict Storage data if needed

Memory Overhead (spark.executor.memoryOverhead, default max(384MB, 0.10 √ó executor.memory))
‚îî‚îÄ‚îÄ Off-heap memory, thread stacks, NIO, interned strings</code></pre>

<hr>

<h3>Common Interview Coding Questions</h3>

<h4>Q: Find the second highest salary per department.</h4>
<pre><code>window_spec = Window.partitionBy(&quot;department&quot;).orderBy(F.desc(&quot;salary&quot;))
df.withColumn(&quot;rank&quot;, F.dense_rank().over(window_spec)) \
  .filter(F.col(&quot;rank&quot;) == 2) \
  .select(&quot;department&quot;, &quot;name&quot;, &quot;salary&quot;) \
  .show()</code></pre>

<h4>Q: Remove duplicate rows keeping the latest record.</h4>
<pre><code>window = Window.partitionBy(&quot;user_id&quot;).orderBy(F.desc(&quot;updated_at&quot;))
df_deduped = df.withColumn(&quot;rn&quot;, F.row_number().over(window)) \
    .filter(F.col(&quot;rn&quot;) == 1) \
    .drop(&quot;rn&quot;)</code></pre>

<h4>Q: Pivot data &mdash; rows to columns.</h4>
<pre><code># Total sales per product per quarter
sales.groupBy(&quot;product&quot;) \
    .pivot(&quot;quarter&quot;, [&quot;Q1&quot;, &quot;Q2&quot;, &quot;Q3&quot;, &quot;Q4&quot;]) \
    .agg(F.sum(&quot;amount&quot;)) \
    .show()

# Unpivot (melt) &mdash; columns to rows
from pyspark.sql.functions import expr
df.selectExpr(&quot;product&quot;, &quot;stack(4, 'Q1', Q1, 'Q2', Q2, 'Q3', Q3, 'Q4', Q4) as (quarter, amount)&quot;)</code></pre>

<h4>Q: Explode nested arrays/structs.</h4>
<pre><code># Array column &rarr; one row per element
df.select(&quot;id&quot;, F.explode(&quot;tags&quot;).alias(&quot;tag&quot;))

# Nested struct &rarr; flatten
df.select(&quot;id&quot;, &quot;address.city&quot;, &quot;address.zip&quot;)
df.select(&quot;id&quot;, F.col(&quot;address.*&quot;))  # Expand all struct fields

# Array of structs
df.select(&quot;id&quot;, F.explode(&quot;orders&quot;).alias(&quot;order&quot;)) \
  .select(&quot;id&quot;, &quot;order.product&quot;, &quot;order.amount&quot;)

# Posexplode &mdash; includes position index
df.select(&quot;id&quot;, F.posexplode(&quot;tags&quot;).alias(&quot;position&quot;, &quot;tag&quot;))</code></pre>

<h4>Q: Sessionize user clickstream data.</h4>
<pre><code># Define session boundary: gap &gt; 30 minutes = new session
window_user = Window.partitionBy(&quot;user_id&quot;).orderBy(&quot;timestamp&quot;)

df_with_gap = df.withColumn(
    &quot;prev_ts&quot;, F.lag(&quot;timestamp&quot;).over(window_user)
).withColumn(
    &quot;gap_minutes&quot;, (F.col(&quot;timestamp&quot;).cast(&quot;long&quot;) - F.col(&quot;prev_ts&quot;).cast(&quot;long&quot;)) / 60
).withColumn(
    &quot;new_session&quot;, F.when(
        (F.col(&quot;gap_minutes&quot;) &gt; 30) | F.col(&quot;gap_minutes&quot;).isNull(), 1
    ).otherwise(0)
)

# Assign session IDs using cumulative sum
session_window = Window.partitionBy(&quot;user_id&quot;).orderBy(&quot;timestamp&quot;) \
    .rowsBetween(Window.unboundedPreceding, Window.currentRow)

df_sessions = df_with_gap.withColumn(
    &quot;session_id&quot;, F.concat(F.col(&quot;user_id&quot;), F.lit(&quot;_&quot;), F.sum(&quot;new_session&quot;).over(session_window))
)</code></pre>

<h4>Q: Implement SCD Type 2 (Slowly Changing Dimension).</h4>
<pre><code>from pyspark.sql.functions import current_timestamp, lit

# existing = current dimension table, incoming = new/changed records
existing = spark.read.parquet(&quot;dim_customer/&quot;)
incoming = spark.read.parquet(&quot;staging/customers/&quot;)

# Find changed records
changed = incoming.join(existing.filter(F.col(&quot;is_current&quot;) == True),
    on=&quot;customer_id&quot;, how=&quot;inner&quot;
).filter(
    (incoming[&quot;name&quot;] != existing[&quot;name&quot;]) |
    (incoming[&quot;address&quot;] != existing[&quot;address&quot;])
).select(incoming[&quot;*&quot;])

# Close old records
closed = existing.join(changed.select(&quot;customer_id&quot;), on=&quot;customer_id&quot;, how=&quot;left_semi&quot;) \
    .withColumn(&quot;is_current&quot;, lit(False)) \
    .withColumn(&quot;end_date&quot;, current_timestamp())

# New versions of changed records
new_versions = changed \
    .withColumn(&quot;is_current&quot;, lit(True)) \
    .withColumn(&quot;start_date&quot;, current_timestamp()) \
    .withColumn(&quot;end_date&quot;, lit(None).cast(&quot;timestamp&quot;))

# New customers (not in existing)
brand_new = incoming.join(existing.select(&quot;customer_id&quot;).distinct(),
    on=&quot;customer_id&quot;, how=&quot;left_anti&quot;) \
    .withColumn(&quot;is_current&quot;, lit(True)) \
    .withColumn(&quot;start_date&quot;, current_timestamp()) \
    .withColumn(&quot;end_date&quot;, lit(None).cast(&quot;timestamp&quot;))

# Unchanged records
unchanged = existing.join(
    changed.select(&quot;customer_id&quot;).union(brand_new.select(&quot;customer_id&quot;)),
    on=&quot;customer_id&quot;, how=&quot;left_anti&quot;
)

# Final dimension table
final_dim = unchanged.unionByName(closed).unionByName(new_versions).unionByName(brand_new)
final_dim.write.mode(&quot;overwrite&quot;).parquet(&quot;dim_customer/&quot;)</code></pre>

<hr>

<h3>Key Concepts Quick Reference</h3>

<h4>Q: What is the difference between <code>repartition()</code> and <code>coalesce()</code>?</h4>
<table>
    <tr><th></th><th><code>repartition(n)</code></th><th><code>coalesce(n)</code></th></tr>
    <tr><td>Shuffle</td><td>Yes (full shuffle)</td><td>No (merges partitions locally)</td></tr>
    <tr><td>Can increase partitions</td><td>Yes</td><td>No (only decrease)</td></tr>
    <tr><td>Data distribution</td><td>Even</td><td>Uneven (combines adjacent)</td></tr>
    <tr><td>Use when</td><td>Increasing partitions, distributing by key</td><td>Reducing partitions before write</td></tr>
</table>

<h4>Q: What is predicate pushdown?</h4>
<p>Predicate pushdown pushes filter conditions down to the data source level so only matching rows are read from disk/S3. It works with Parquet, ORC, JDBC, and Delta Lake.</p>
<pre><code># With Parquet, this only reads row groups where age &gt; 30
spark.read.parquet(&quot;data.parquet&quot;).filter(F.col(&quot;age&quot;) &gt; 30)

# Partition pruning &mdash; only reads partition directories matching the filter
spark.read.parquet(&quot;data/year=2024/month=*/&quot;).filter(F.col(&quot;year&quot;) == 2024)</code></pre>

<h4>Q: What is speculative execution?</h4>
<p>Spark can launch backup copies of slow-running tasks on different executors. Whichever copy finishes first wins.</p>
<pre><code>spark.conf.set(&quot;spark.speculation&quot;, True)
spark.conf.set(&quot;spark.speculation.multiplier&quot;, 1.5)  # Task 1.5x slower than median
spark.conf.set(&quot;spark.speculation.quantile&quot;, 0.75)   # 75% of tasks must complete first</code></pre>

<h4>Q: Explain <code>cache()</code> vs <code>persist()</code> vs <code>checkpoint()</code>.</h4>
<table>
    <tr><th>Method</th><th>Storage</th><th>Lineage</th><th>Use Case</th></tr>
    <tr><td><code>cache()</code></td><td>Memory only</td><td>Preserved</td><td>Quick reuse of DataFrame</td></tr>
    <tr><td><code>persist(level)</code></td><td>Memory, disk, or both</td><td>Preserved</td><td>Control storage level</td></tr>
    <tr><td><code>checkpoint()</code></td><td>Disk (reliable storage)</td><td>Truncated</td><td>Break long lineage chains, fault tolerance</td></tr>
</table>

<pre><code># Checkpoint breaks the DAG lineage &mdash; useful for iterative algorithms
spark.sparkContext.setCheckpointDir(&quot;/tmp/checkpoints&quot;)
df.checkpoint()  # Materializes and truncates lineage

# Local checkpoint &mdash; faster but not fault-tolerant
df.localCheckpoint()</code></pre>

<h4>Q: What are Accumulators and Broadcast Variables?</h4>
<pre><code># Accumulators &mdash; write-only shared variables for counters/sums
error_count = spark.sparkContext.accumulator(0)

def process_row(row):
    global error_count
    if row[&quot;status&quot;] == &quot;ERROR&quot;:
        error_count.add(1)
    return row

df.rdd.foreach(process_row)
print(f&quot;Errors found: {error_count.value}&quot;)

# Broadcast Variables &mdash; read-only shared lookup data
lookup_dict = {&quot;NY&quot;: &quot;New York&quot;, &quot;CA&quot;: &quot;California&quot;, &quot;TX&quot;: &quot;Texas&quot;}
broadcast_lookup = spark.sparkContext.broadcast(lookup_dict)

@F.udf(StringType())
def resolve_state(code):
    return broadcast_lookup.value.get(code, &quot;Unknown&quot;)

df.withColumn(&quot;state_name&quot;, resolve_state(F.col(&quot;state_code&quot;)))</code></pre>

<h4>Q: What is Adaptive Query Execution (AQE)?</h4>
<p>AQE (Spark 3.0+) optimizes queries at runtime based on statistics collected during execution:</p>
<ol>
    <li><strong>Dynamically coalesce shuffle partitions</strong>: Combines small post-shuffle partitions</li>
    <li><strong>Dynamically switch join strategies</strong>: Switches to broadcast join if a table is smaller than expected</li>
    <li><strong>Dynamically handle skewed joins</strong>: Splits skewed partitions into smaller sub-partitions</li>
</ol>
<pre><code>spark.conf.set(&quot;spark.sql.adaptive.enabled&quot;, True)
spark.conf.set(&quot;spark.sql.adaptive.coalescePartitions.enabled&quot;, True)
spark.conf.set(&quot;spark.sql.adaptive.skewJoin.enabled&quot;, True)
spark.conf.set(&quot;spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes&quot;, &quot;256MB&quot;)</code></pre>

<hr>

<h3>Common Mistakes to Avoid</h3>
<div class="warning">
    <ol>
        <li><strong>Using <code>collect()</code> on large DataFrames</strong> &mdash; brings all data to driver, causes OOM</li>
        <li><strong>Not caching reused DataFrames</strong> &mdash; recomputes from scratch each time</li>
        <li><strong>Using Python UDFs instead of built-in functions</strong> &mdash; UDFs serialize data to Python and back (10-100x slower)</li>
        <li><strong>Not repartitioning after heavy filtering</strong> &mdash; leaves many empty partitions</li>
        <li><strong>Using <code>count()</code> just to check if empty</strong> &mdash; use <code>df.head(1)</code> or <code>df.isEmpty()</code> instead</li>
        <li><strong>Ignoring data skew</strong> &mdash; one large partition blocks entire stage</li>
        <li><strong>Reading with <code>inferSchema=True</code> in production</strong> &mdash; requires extra pass over data; define schemas explicitly</li>
        <li><strong>Calling actions inside loops</strong> &mdash; each action triggers full DAG re-evaluation</li>
        <li><strong>Not specifying partition columns when writing</strong> &mdash; leads to slow reads on large datasets</li>
        <li><strong>Using <code>orderBy()</code> globally</strong> &mdash; shuffles all data to single partition; use Window functions instead</li>
    </ol>
</div>

<hr>
<p style="text-align: center; color: #666; margin-top: 40px;">
    <em>PySpark Cheat Sheet &mdash; For Data Engineers &amp; Data Scientists</em>
</p>

</body>
</html>
